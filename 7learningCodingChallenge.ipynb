{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query for downloading the data from Google Cloud BIGQUERY"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT\n",
    "  *,\n",
    "  FORMAT_TIMESTAMP('%Y-%m-%d', TIMESTAMP(CONCAT(CAST(year AS STRING), '-', CAST(month AS STRING), '-', CAST(day AS STRING)))) AS formatted_date\n",
    "FROM\n",
    "  `bigquery-public-data.samples.gsod`\n",
    "WHERE\n",
    "  year BETWEEN 2005 AND 2009\n",
    "  AND station_number BETWEEN 725300 AND 726300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The dataset was downloaded as csv and later uploaded in Visual Studio where futher processing was performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_576821/1797354241.py:2: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv('/root/code/thesis/codeFolder/LatestDataInUse/csv/7learnings.csv')\n"
     ]
    }
   ],
   "source": [
    "#importing dataset by reading csv file (The data was downloaded from Google Cloud using BIGQUERY)\n",
    "df=pd.read_csv('/root/code/thesis/codeFolder/LatestDataInUse/csv/7learnings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_number</th>\n",
       "      <th>wban_number</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>mean_temp</th>\n",
       "      <th>num_mean_temp_samples</th>\n",
       "      <th>mean_dew_point</th>\n",
       "      <th>num_mean_dew_point_samples</th>\n",
       "      <th>mean_sealevel_pressure</th>\n",
       "      <th>...</th>\n",
       "      <th>min_temperature_explicit</th>\n",
       "      <th>total_precipitation</th>\n",
       "      <th>snow_depth</th>\n",
       "      <th>fog</th>\n",
       "      <th>rain</th>\n",
       "      <th>snow</th>\n",
       "      <th>hail</th>\n",
       "      <th>thunder</th>\n",
       "      <th>tornado</th>\n",
       "      <th>formatted_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>725940</td>\n",
       "      <td>99999</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>51.299999</td>\n",
       "      <td>4</td>\n",
       "      <td>45.299999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1013.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2005-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>725940</td>\n",
       "      <td>99999</td>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1025.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2005-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>725940</td>\n",
       "      <td>99999</td>\n",
       "      <td>2005</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>45.700001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1021.799988</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2005-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>725869</td>\n",
       "      <td>99999</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>30.200001</td>\n",
       "      <td>5</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2005-01-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>725827</td>\n",
       "      <td>99999</td>\n",
       "      <td>2005</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>28.799999</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1015.599976</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2005-10-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_number  wban_number  year  month  day  mean_temp  \\\n",
       "0          725940        99999  2005      1   25  51.299999   \n",
       "1          725940        99999  2005      3    7  51.500000   \n",
       "2          725940        99999  2005      5   11  51.000000   \n",
       "3          725869        99999  2005      1   27  30.200001   \n",
       "4          725827        99999  2005     10   23  55.000000   \n",
       "\n",
       "   num_mean_temp_samples  mean_dew_point  num_mean_dew_point_samples  \\\n",
       "0                      4       45.299999                         4.0   \n",
       "1                      4       48.000000                         4.0   \n",
       "2                      4       45.700001                         4.0   \n",
       "3                      5       26.600000                         5.0   \n",
       "4                      5       28.799999                         5.0   \n",
       "\n",
       "   mean_sealevel_pressure  ...  min_temperature_explicit  total_precipitation  \\\n",
       "0             1013.500000  ...                       NaN                 0.00   \n",
       "1             1025.000000  ...                       NaN                 0.00   \n",
       "2             1021.799988  ...                       NaN                 0.01   \n",
       "3                     NaN  ...                       NaN                 0.00   \n",
       "4             1015.599976  ...                       NaN                 0.00   \n",
       "\n",
       "   snow_depth    fog   rain   snow   hail  thunder  tornado  formatted_date  \n",
       "0         NaN  False  False  False  False    False    False      2005-01-25  \n",
       "1         NaN  False  False  False  False    False    False      2005-03-07  \n",
       "2         NaN  False  False  False  False    False    False      2005-05-11  \n",
       "3         NaN  False  False  False  False    False    False      2005-01-27  \n",
       "4         NaN  False  False  False  False    False    False      2005-10-23  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32842, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering stations 725300 to 725330 that have information from 2005 till 2009.\n",
    "df = df[(df['station_number'] >= 725300) & (df['station_number'] <= 725330)]\n",
    "\n",
    "# Specify the columns to drop \n",
    "# Some of these column like were irrelevant, some coulumns had missing values\n",
    "columns_to_drop = ['min_temperature', 'min_temperature_explicit','mean_station_pressure','mean_sealevel_pressure', 'num_mean_station_pressure_samples','year','month','day','snow_depth', 'num_mean_sealevel_pressure_samples', 'wban_number', 'num_mean_temp_samples','num_mean_dew_point_samples', 'num_mean_visibility_samples' ,'max_sustained_wind_speed','max_gust_wind_speed','max_temperature_explicit', 'num_mean_wind_speed_samples','tornado','max_temperature']\n",
    "\n",
    "# Drop the specified columns\n",
    "df_dropped = df.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean_wind_speed', 'total_precipitation']\n"
     ]
    }
   ],
   "source": [
    "# Checking columns with NAN values\n",
    "columns_with_nan = df_dropped.columns[df_dropped.isna().any()].tolist()\n",
    "\n",
    "print(columns_with_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies for dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Strategy 1: Filling the missing values with mean values\n",
    "#data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "### Strategy 2: Interpolating the missing values\n",
    "#df_dropped['mean_wind_speed'] = df_dropped['mean_wind_speed'].interpolate()\n",
    "#df_dropped['total_precipitation'] = df_dropped['total_precipitation'].interpolate()\n",
    "#df_dropped['mean_sealevel_pressure'] = df_dropped['mean_sealevel_pressure'].interpolate()\n",
    "\n",
    "### Strategy 3: Droping the missing rows\n",
    "df_dropped=df_dropped.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Droping the rows with missing values reduces data size but other strategies are more likely to compromise data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dataframe with respect to data and resting the dataframe index\n",
    "final_df = df_dropped.sort_values(by='formatted_date') \n",
    "final_df = final_df.set_index('formatted_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_number</th>\n",
       "      <th>mean_temp</th>\n",
       "      <th>mean_dew_point</th>\n",
       "      <th>mean_visibility</th>\n",
       "      <th>mean_wind_speed</th>\n",
       "      <th>total_precipitation</th>\n",
       "      <th>fog</th>\n",
       "      <th>rain</th>\n",
       "      <th>snow</th>\n",
       "      <th>hail</th>\n",
       "      <th>thunder</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formatted_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-02</th>\n",
       "      <td>725314</td>\n",
       "      <td>60.799999</td>\n",
       "      <td>57.599998</td>\n",
       "      <td>8.9</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>725300</td>\n",
       "      <td>32.299999</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>7.7</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.34</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>725330</td>\n",
       "      <td>24.799999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>9.7</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.03</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>725305</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>8.4</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-08</th>\n",
       "      <td>725316</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>7.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                station_number  mean_temp  mean_dew_point  mean_visibility  \\\n",
       "formatted_date                                                               \n",
       "2005-01-02              725314  60.799999       57.599998              8.9   \n",
       "2005-01-04              725300  32.299999       29.400000              7.7   \n",
       "2005-01-07              725330  24.799999       20.400000              9.7   \n",
       "2005-01-07              725305  16.700001       13.100000              8.4   \n",
       "2005-01-08              725316  28.600000       25.500000              7.3   \n",
       "\n",
       "                mean_wind_speed  total_precipitation    fog   rain   snow  \\\n",
       "formatted_date                                                              \n",
       "2005-01-02                  4.8                 0.40  False  False  False   \n",
       "2005-01-04                  9.3                 0.34   True   True   True   \n",
       "2005-01-07                  7.7                 0.03  False  False  False   \n",
       "2005-01-07                  6.9                 0.01  False  False  False   \n",
       "2005-01-08                  5.4                 0.10   True   True   True   \n",
       "\n",
       "                 hail  thunder  \n",
       "formatted_date                  \n",
       "2005-01-02      False    False  \n",
       "2005-01-04       True     True  \n",
       "2005-01-07      False    False  \n",
       "2005-01-07      False    False  \n",
       "2005-01-08       True     True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['station_number', 'mean_temp', 'mean_dew_point', 'mean_visibility',\n",
       "       'mean_wind_speed', 'total_precipitation', 'fog', 'rain', 'snow', 'hail',\n",
       "       'thunder'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the categorical values to numerical values\n",
    "final_df['fog'] = final_df['fog'].astype(int)\n",
    "final_df['rain'] = final_df['rain'].astype(int)\n",
    "final_df['hail'] = final_df['hail'].astype(int)\n",
    "final_df['thunder'] = final_df['thunder'].astype(int)\n",
    "final_df['snow'] = final_df['snow'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_number</th>\n",
       "      <th>mean_temp</th>\n",
       "      <th>mean_dew_point</th>\n",
       "      <th>mean_visibility</th>\n",
       "      <th>mean_wind_speed</th>\n",
       "      <th>total_precipitation</th>\n",
       "      <th>fog</th>\n",
       "      <th>rain</th>\n",
       "      <th>snow</th>\n",
       "      <th>hail</th>\n",
       "      <th>thunder</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formatted_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-02</th>\n",
       "      <td>725314</td>\n",
       "      <td>60.799999</td>\n",
       "      <td>57.599998</td>\n",
       "      <td>8.9</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>725300</td>\n",
       "      <td>32.299999</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>7.7</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>725330</td>\n",
       "      <td>24.799999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>9.7</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>725305</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>8.4</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-08</th>\n",
       "      <td>725316</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>7.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                station_number  mean_temp  mean_dew_point  mean_visibility  \\\n",
       "formatted_date                                                               \n",
       "2005-01-02              725314  60.799999       57.599998              8.9   \n",
       "2005-01-04              725300  32.299999       29.400000              7.7   \n",
       "2005-01-07              725330  24.799999       20.400000              9.7   \n",
       "2005-01-07              725305  16.700001       13.100000              8.4   \n",
       "2005-01-08              725316  28.600000       25.500000              7.3   \n",
       "\n",
       "                mean_wind_speed  total_precipitation  fog  rain  snow  hail  \\\n",
       "formatted_date                                                                \n",
       "2005-01-02                  4.8                 0.40    0     0     0     0   \n",
       "2005-01-04                  9.3                 0.34    1     1     1     1   \n",
       "2005-01-07                  7.7                 0.03    0     0     0     0   \n",
       "2005-01-07                  6.9                 0.01    0     0     0     0   \n",
       "2005-01-08                  5.4                 0.10    1     1     1     1   \n",
       "\n",
       "                thunder  \n",
       "formatted_date           \n",
       "2005-01-02            0  \n",
       "2005-01-04            1  \n",
       "2005-01-07            0  \n",
       "2005-01-07            0  \n",
       "2005-01-08            1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([725300, 725305, 725314, 725315, 725316, 725317, 725320, 725326,\n",
       "       725327, 725330])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(final_df['station_number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset has huge class imbalance with 1350 values representing class 0 \"no snow\" and only 174 values prepresenting class 1 \"snow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1350\n",
       "1     174\n",
       "Name: snow, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['snow'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   station_number  counts\n",
      "0          725300     142\n",
      "1          725305     149\n",
      "2          725314     150\n",
      "3          725315     146\n",
      "4          725316     150\n",
      "5          725317     161\n",
      "6          725320     148\n",
      "7          725326     172\n",
      "8          725327     153\n",
      "9          725330     153\n"
     ]
    }
   ],
   "source": [
    "# Group by 'station_number' and count the number of entries for each station\n",
    "station_counts = final_df.groupby('station_number').size().reset_index(name='counts')\n",
    "\n",
    "# Display the station counts\n",
    "print(station_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the data by station number and date\n",
    "final_df = final_df.sort_values(by=['station_number', 'formatted_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_number</th>\n",
       "      <th>mean_temp</th>\n",
       "      <th>mean_dew_point</th>\n",
       "      <th>mean_visibility</th>\n",
       "      <th>mean_wind_speed</th>\n",
       "      <th>total_precipitation</th>\n",
       "      <th>fog</th>\n",
       "      <th>rain</th>\n",
       "      <th>snow</th>\n",
       "      <th>hail</th>\n",
       "      <th>thunder</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formatted_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>725300</td>\n",
       "      <td>32.299999</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>7.7</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-15</th>\n",
       "      <td>725300</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>-7.200000</td>\n",
       "      <td>9.9</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-23</th>\n",
       "      <td>725300</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-28</th>\n",
       "      <td>725300</td>\n",
       "      <td>14.300000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>9.9</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-02-01</th>\n",
       "      <td>725300</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>23.700001</td>\n",
       "      <td>8.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-02-04</th>\n",
       "      <td>725300</td>\n",
       "      <td>35.299999</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-02-07</th>\n",
       "      <td>725300</td>\n",
       "      <td>43.200001</td>\n",
       "      <td>41.400002</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-02-08</th>\n",
       "      <td>725300</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>27.200001</td>\n",
       "      <td>8.9</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-02-20</th>\n",
       "      <td>725300</td>\n",
       "      <td>34.099998</td>\n",
       "      <td>29.100000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-02-21</th>\n",
       "      <td>725300</td>\n",
       "      <td>35.099998</td>\n",
       "      <td>30.700001</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-02-24</th>\n",
       "      <td>725300</td>\n",
       "      <td>31.100000</td>\n",
       "      <td>22.799999</td>\n",
       "      <td>9.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-02-25</th>\n",
       "      <td>725300</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>7.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-31</th>\n",
       "      <td>725300</td>\n",
       "      <td>51.299999</td>\n",
       "      <td>41.799999</td>\n",
       "      <td>9.4</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-01</th>\n",
       "      <td>725300</td>\n",
       "      <td>46.099998</td>\n",
       "      <td>34.799999</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-10</th>\n",
       "      <td>725300</td>\n",
       "      <td>60.400002</td>\n",
       "      <td>39.700001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-29</th>\n",
       "      <td>725300</td>\n",
       "      <td>46.099998</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>9.9</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-05-15</th>\n",
       "      <td>725300</td>\n",
       "      <td>49.299999</td>\n",
       "      <td>37.700001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-05-16</th>\n",
       "      <td>725300</td>\n",
       "      <td>50.700001</td>\n",
       "      <td>36.200001</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-05-19</th>\n",
       "      <td>725300</td>\n",
       "      <td>63.799999</td>\n",
       "      <td>53.700001</td>\n",
       "      <td>6.2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-05-23</th>\n",
       "      <td>725300</td>\n",
       "      <td>64.300003</td>\n",
       "      <td>47.799999</td>\n",
       "      <td>9.9</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                station_number  mean_temp  mean_dew_point  mean_visibility  \\\n",
       "formatted_date                                                               \n",
       "2005-01-04              725300  32.299999       29.400000              7.7   \n",
       "2005-01-15              725300   8.400000       -7.200000              9.9   \n",
       "2005-01-23              725300  14.100000        5.100000              8.0   \n",
       "2005-01-28              725300  14.300000        2.800000              9.9   \n",
       "2005-02-01              725300  30.500000       23.700001              8.8   \n",
       "2005-02-04              725300  35.299999       29.500000              4.9   \n",
       "2005-02-07              725300  43.200001       41.400002              3.8   \n",
       "2005-02-08              725300  31.400000       27.200001              8.9   \n",
       "2005-02-20              725300  34.099998       29.100000              4.0   \n",
       "2005-02-21              725300  35.099998       30.700001              8.4   \n",
       "2005-02-24              725300  31.100000       22.799999              9.2   \n",
       "2005-02-25              725300  33.500000       25.900000              7.3   \n",
       "2005-03-31              725300  51.299999       41.799999              9.4   \n",
       "2005-04-01              725300  46.099998       34.799999              9.3   \n",
       "2005-04-10              725300  60.400002       39.700001             10.0   \n",
       "2005-04-29              725300  46.099998       26.100000              9.9   \n",
       "2005-05-15              725300  49.299999       37.700001             10.0   \n",
       "2005-05-16              725300  50.700001       36.200001              9.9   \n",
       "2005-05-19              725300  63.799999       53.700001              6.2   \n",
       "2005-05-23              725300  64.300003       47.799999              9.9   \n",
       "\n",
       "                mean_wind_speed  total_precipitation  fog  rain  snow  hail  \\\n",
       "formatted_date                                                                \n",
       "2005-01-04                  9.3                 0.34    1     1     1     1   \n",
       "2005-01-15                  7.7                 0.00    0     0     0     0   \n",
       "2005-01-23                 10.3                 0.22    0     0     0     0   \n",
       "2005-01-28                  8.2                 0.02    0     0     0     0   \n",
       "2005-02-01                  4.0                 0.00    1     1     1     1   \n",
       "2005-02-04                  4.0                 0.00    1     1     1     1   \n",
       "2005-02-07                  5.9                 0.14    1     1     1     1   \n",
       "2005-02-08                 10.1                 0.06    1     1     1     1   \n",
       "2005-02-20                  8.6                 0.30    1     1     1     1   \n",
       "2005-02-21                  7.8                 0.01    1     1     1     1   \n",
       "2005-02-24                  4.0                 0.00    0     0     0     0   \n",
       "2005-02-25                  5.7                 0.00    0     0     0     0   \n",
       "2005-03-31                 14.5                 0.62    1     1     1     1   \n",
       "2005-04-01                  5.4                 0.00    1     1     1     1   \n",
       "2005-04-10                  6.2                 0.00    0     0     0     0   \n",
       "2005-04-29                  7.2                 0.00    0     0     0     0   \n",
       "2005-05-15                 10.8                 0.00    0     0     0     0   \n",
       "2005-05-16                  6.0                 0.00    0     0     0     0   \n",
       "2005-05-19                  8.5                 1.20    1     1     1     1   \n",
       "2005-05-23                 11.2                 0.00    0     0     0     0   \n",
       "\n",
       "                thunder  \n",
       "formatted_date           \n",
       "2005-01-04            1  \n",
       "2005-01-15            0  \n",
       "2005-01-23            0  \n",
       "2005-01-28            0  \n",
       "2005-02-01            1  \n",
       "2005-02-04            1  \n",
       "2005-02-07            1  \n",
       "2005-02-08            1  \n",
       "2005-02-20            1  \n",
       "2005-02-21            1  \n",
       "2005-02-24            0  \n",
       "2005-02-25            0  \n",
       "2005-03-31            1  \n",
       "2005-04-01            1  \n",
       "2005-04-10            0  \n",
       "2005-04-29            0  \n",
       "2005-05-15            0  \n",
       "2005-05-16            0  \n",
       "2005-05-19            1  \n",
       "2005-05-23            0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['snow_tomorrow'] = final_df.groupby('station_number')['snow'].shift(-1)\n",
    "final_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_number</th>\n",
       "      <th>mean_temp</th>\n",
       "      <th>mean_dew_point</th>\n",
       "      <th>mean_visibility</th>\n",
       "      <th>mean_wind_speed</th>\n",
       "      <th>total_precipitation</th>\n",
       "      <th>fog</th>\n",
       "      <th>rain</th>\n",
       "      <th>snow</th>\n",
       "      <th>hail</th>\n",
       "      <th>thunder</th>\n",
       "      <th>snow_tomorrow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formatted_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>725300</td>\n",
       "      <td>32.299999</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>7.7</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-15</th>\n",
       "      <td>725300</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>-7.200000</td>\n",
       "      <td>9.9</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-23</th>\n",
       "      <td>725300</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-28</th>\n",
       "      <td>725300</td>\n",
       "      <td>14.300000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>9.9</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-02-01</th>\n",
       "      <td>725300</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>23.700001</td>\n",
       "      <td>8.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-11-13</th>\n",
       "      <td>725330</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>33.299999</td>\n",
       "      <td>9.5</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-12-09</th>\n",
       "      <td>725330</td>\n",
       "      <td>34.599998</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.7</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-12-15</th>\n",
       "      <td>725330</td>\n",
       "      <td>33.299999</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>8.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-12-21</th>\n",
       "      <td>725330</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-12-24</th>\n",
       "      <td>725330</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>9.8</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1514 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                station_number  mean_temp  mean_dew_point  mean_visibility  \\\n",
       "formatted_date                                                               \n",
       "2005-01-04              725300  32.299999       29.400000              7.7   \n",
       "2005-01-15              725300   8.400000       -7.200000              9.9   \n",
       "2005-01-23              725300  14.100000        5.100000              8.0   \n",
       "2005-01-28              725300  14.300000        2.800000              9.9   \n",
       "2005-02-01              725300  30.500000       23.700001              8.8   \n",
       "...                        ...        ...             ...              ...   \n",
       "2009-11-13              725330  42.599998       33.299999              9.5   \n",
       "2009-12-09              725330  34.599998       30.000000              5.7   \n",
       "2009-12-15              725330  33.299999       28.600000              8.9   \n",
       "2009-12-21              725330  28.400000       25.400000              3.9   \n",
       "2009-12-24              725330  28.400000       22.000000              9.8   \n",
       "\n",
       "                mean_wind_speed  total_precipitation  fog  rain  snow  hail  \\\n",
       "formatted_date                                                                \n",
       "2005-01-04                  9.3                 0.34    1     1     1     1   \n",
       "2005-01-15                  7.7                 0.00    0     0     0     0   \n",
       "2005-01-23                 10.3                 0.22    0     0     0     0   \n",
       "2005-01-28                  8.2                 0.02    0     0     0     0   \n",
       "2005-02-01                  4.0                 0.00    1     1     1     1   \n",
       "...                         ...                  ...  ...   ...   ...   ...   \n",
       "2009-11-13                  4.4                 0.00    0     0     0     0   \n",
       "2009-12-09                 19.9                 0.57    0     0     0     0   \n",
       "2009-12-15                 12.2                 0.01    0     0     0     0   \n",
       "2009-12-21                  5.6                 0.01    0     0     0     0   \n",
       "2009-12-24                 15.6                 0.01    0     0     0     0   \n",
       "\n",
       "                thunder  snow_tomorrow  \n",
       "formatted_date                          \n",
       "2005-01-04            1            0.0  \n",
       "2005-01-15            0            0.0  \n",
       "2005-01-23            0            0.0  \n",
       "2005-01-28            0            1.0  \n",
       "2005-02-01            1            1.0  \n",
       "...                 ...            ...  \n",
       "2009-11-13            0            0.0  \n",
       "2009-12-09            0            0.0  \n",
       "2009-12-15            0            0.0  \n",
       "2009-12-21            0            0.0  \n",
       "2009-12-24            0            0.0  \n",
       "\n",
       "[1514 rows x 12 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['station_number', 'mean_temp', 'mean_dew_point', 'mean_visibility',\n",
       "       'mean_wind_speed', 'total_precipitation', 'fog', 'rain', 'snow', 'hail',\n",
       "       'thunder', 'snow_tomorrow'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split, ensuring the split is done per station to maintain the time series nature\n",
    "def train_test_split_per_station(df, test_size=0.2):\n",
    "    train_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "    for station in df['station_number'].unique():\n",
    "        station_data = df[df['station_number'] == station]\n",
    "        train_station, test_station = train_test_split(station_data, test_size=test_size, shuffle=False)\n",
    "        train_data = pd.concat([train_data, train_station])\n",
    "        test_data = pd.concat([test_data, test_station])\n",
    "    return train_data, test_data\n",
    "\n",
    "features= ['station_number', 'mean_temp', 'mean_dew_point', 'mean_visibility',\n",
    "       'mean_wind_speed', 'total_precipitation', 'fog', 'rain', 'snow', 'hail',\n",
    "       'thunder',]\n",
    "\n",
    "train_data, test_data = train_test_split_per_station(final_df)\n",
    "X_train = train_data[features].values\n",
    "y_train = train_data['snow_tomorrow'].values\n",
    "X_test = test_data[features].values\n",
    "y_test = test_data['snow_tomorrow'].values\n",
    "\n",
    "# Normalizing the features to ensure equal weightage of all the variables\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "#Converting to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # unsqueeze to add a dimension\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the neural network model\n",
    "class SnowForecastModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SnowForecastModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(len(features), 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(preds, labels):\n",
    "    predicted = preds.round()\n",
    "    correct = (predicted == labels).float()\n",
    "    accuracy = correct.sum() / len(correct)\n",
    "    return accuracy*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y_train is a numpy array\n",
    "if isinstance(y_train, torch.Tensor):\n",
    "    y_train_copy = y_train.cpu().numpy()\n",
    "\n",
    "\n",
    "# Flatten y_train to ensure it's a 1D array\n",
    "y_train_flat = y_train_copy.flatten()\n",
    "\n",
    "# Convert to int\n",
    "y_train_int = y_train_flat.astype(int)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_int), y=y_train_int)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# Use class weights in the loss function\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "# Instantiating the model, defining the loss function and the optimizer\n",
    "model = SnowForecastModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "\n",
    "# Lists to store losses and accuracies\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Train Loss: 1.0952, Train Accuracy: 74.3993, Test Loss: 1.0792, Test Accuracy: 75.8958\n",
      "Epoch [2/1000], Train Loss: 1.0950, Train Accuracy: 74.8964, Test Loss: 1.0790, Test Accuracy: 76.8730\n",
      "Epoch [3/1000], Train Loss: 1.0948, Train Accuracy: 75.7249, Test Loss: 1.0788, Test Accuracy: 76.5472\n",
      "Epoch [4/1000], Train Loss: 1.0947, Train Accuracy: 76.3049, Test Loss: 1.0786, Test Accuracy: 77.5244\n",
      "Epoch [5/1000], Train Loss: 1.0945, Train Accuracy: 76.8020, Test Loss: 1.0784, Test Accuracy: 77.5244\n",
      "Epoch [6/1000], Train Loss: 1.0944, Train Accuracy: 77.4648, Test Loss: 1.0783, Test Accuracy: 78.1759\n",
      "Epoch [7/1000], Train Loss: 1.0942, Train Accuracy: 77.8790, Test Loss: 1.0781, Test Accuracy: 79.1531\n",
      "Epoch [8/1000], Train Loss: 1.0941, Train Accuracy: 78.2104, Test Loss: 1.0779, Test Accuracy: 79.8046\n",
      "Epoch [9/1000], Train Loss: 1.0939, Train Accuracy: 79.2046, Test Loss: 1.0777, Test Accuracy: 80.4560\n",
      "Epoch [10/1000], Train Loss: 1.0938, Train Accuracy: 79.7017, Test Loss: 1.0775, Test Accuracy: 81.1075\n",
      "Epoch [11/1000], Train Loss: 1.0936, Train Accuracy: 80.1988, Test Loss: 1.0774, Test Accuracy: 82.0847\n",
      "Epoch [12/1000], Train Loss: 1.0935, Train Accuracy: 80.6959, Test Loss: 1.0772, Test Accuracy: 82.0847\n",
      "Epoch [13/1000], Train Loss: 1.0933, Train Accuracy: 81.6073, Test Loss: 1.0770, Test Accuracy: 83.3876\n",
      "Epoch [14/1000], Train Loss: 1.0931, Train Accuracy: 82.1044, Test Loss: 1.0768, Test Accuracy: 83.7134\n",
      "Epoch [15/1000], Train Loss: 1.0930, Train Accuracy: 82.2701, Test Loss: 1.0767, Test Accuracy: 83.3876\n",
      "Epoch [16/1000], Train Loss: 1.0928, Train Accuracy: 82.9329, Test Loss: 1.0765, Test Accuracy: 83.7134\n",
      "Epoch [17/1000], Train Loss: 1.0927, Train Accuracy: 83.3471, Test Loss: 1.0763, Test Accuracy: 84.0391\n",
      "Epoch [18/1000], Train Loss: 1.0925, Train Accuracy: 83.5128, Test Loss: 1.0761, Test Accuracy: 85.0163\n",
      "Epoch [19/1000], Train Loss: 1.0924, Train Accuracy: 83.8442, Test Loss: 1.0760, Test Accuracy: 85.3420\n",
      "Epoch [20/1000], Train Loss: 1.0922, Train Accuracy: 84.4242, Test Loss: 1.0758, Test Accuracy: 85.6678\n",
      "Epoch [21/1000], Train Loss: 1.0921, Train Accuracy: 84.7556, Test Loss: 1.0756, Test Accuracy: 85.6678\n",
      "Epoch [22/1000], Train Loss: 1.0919, Train Accuracy: 85.0041, Test Loss: 1.0754, Test Accuracy: 86.6450\n",
      "Epoch [23/1000], Train Loss: 1.0918, Train Accuracy: 85.3355, Test Loss: 1.0753, Test Accuracy: 87.2964\n",
      "Epoch [24/1000], Train Loss: 1.0916, Train Accuracy: 85.5012, Test Loss: 1.0751, Test Accuracy: 87.6221\n",
      "Epoch [25/1000], Train Loss: 1.0915, Train Accuracy: 85.9983, Test Loss: 1.0749, Test Accuracy: 87.9479\n",
      "Epoch [26/1000], Train Loss: 1.0913, Train Accuracy: 86.2469, Test Loss: 1.0747, Test Accuracy: 88.2736\n",
      "Epoch [27/1000], Train Loss: 1.0912, Train Accuracy: 86.3297, Test Loss: 1.0746, Test Accuracy: 88.2736\n",
      "Epoch [28/1000], Train Loss: 1.0910, Train Accuracy: 86.5783, Test Loss: 1.0744, Test Accuracy: 88.9251\n",
      "Epoch [29/1000], Train Loss: 1.0909, Train Accuracy: 86.6611, Test Loss: 1.0742, Test Accuracy: 89.5765\n",
      "Epoch [30/1000], Train Loss: 1.0907, Train Accuracy: 86.7440, Test Loss: 1.0740, Test Accuracy: 89.5765\n",
      "Epoch [31/1000], Train Loss: 1.0906, Train Accuracy: 87.1582, Test Loss: 1.0739, Test Accuracy: 89.5765\n",
      "Epoch [32/1000], Train Loss: 1.0904, Train Accuracy: 87.3239, Test Loss: 1.0737, Test Accuracy: 89.5765\n",
      "Epoch [33/1000], Train Loss: 1.0903, Train Accuracy: 87.4068, Test Loss: 1.0735, Test Accuracy: 89.5765\n",
      "Epoch [34/1000], Train Loss: 1.0902, Train Accuracy: 87.6553, Test Loss: 1.0733, Test Accuracy: 89.5765\n",
      "Epoch [35/1000], Train Loss: 1.0900, Train Accuracy: 87.7382, Test Loss: 1.0732, Test Accuracy: 89.5765\n",
      "Epoch [36/1000], Train Loss: 1.0899, Train Accuracy: 87.9039, Test Loss: 1.0730, Test Accuracy: 89.5765\n",
      "Epoch [37/1000], Train Loss: 1.0897, Train Accuracy: 87.9039, Test Loss: 1.0728, Test Accuracy: 89.5765\n",
      "Epoch [38/1000], Train Loss: 1.0896, Train Accuracy: 87.9867, Test Loss: 1.0726, Test Accuracy: 89.5765\n",
      "Epoch [39/1000], Train Loss: 1.0894, Train Accuracy: 88.0696, Test Loss: 1.0725, Test Accuracy: 89.5765\n",
      "Epoch [40/1000], Train Loss: 1.0893, Train Accuracy: 88.2353, Test Loss: 1.0723, Test Accuracy: 89.5765\n",
      "Epoch [41/1000], Train Loss: 1.0891, Train Accuracy: 88.2353, Test Loss: 1.0721, Test Accuracy: 89.5765\n",
      "Epoch [42/1000], Train Loss: 1.0890, Train Accuracy: 88.1524, Test Loss: 1.0720, Test Accuracy: 89.5765\n",
      "Epoch [43/1000], Train Loss: 1.0888, Train Accuracy: 88.1524, Test Loss: 1.0718, Test Accuracy: 89.5765\n",
      "Epoch [44/1000], Train Loss: 1.0887, Train Accuracy: 88.1524, Test Loss: 1.0716, Test Accuracy: 89.5765\n",
      "Epoch [45/1000], Train Loss: 1.0885, Train Accuracy: 88.2353, Test Loss: 1.0714, Test Accuracy: 89.9023\n",
      "Epoch [46/1000], Train Loss: 1.0884, Train Accuracy: 88.2353, Test Loss: 1.0713, Test Accuracy: 89.9023\n",
      "Epoch [47/1000], Train Loss: 1.0882, Train Accuracy: 88.2353, Test Loss: 1.0711, Test Accuracy: 89.9023\n",
      "Epoch [48/1000], Train Loss: 1.0881, Train Accuracy: 88.2353, Test Loss: 1.0709, Test Accuracy: 89.9023\n",
      "Epoch [49/1000], Train Loss: 1.0879, Train Accuracy: 88.2353, Test Loss: 1.0707, Test Accuracy: 89.9023\n",
      "Epoch [50/1000], Train Loss: 1.0878, Train Accuracy: 88.1524, Test Loss: 1.0706, Test Accuracy: 89.9023\n",
      "Epoch [51/1000], Train Loss: 1.0876, Train Accuracy: 88.1524, Test Loss: 1.0704, Test Accuracy: 89.9023\n",
      "Epoch [52/1000], Train Loss: 1.0875, Train Accuracy: 88.1524, Test Loss: 1.0702, Test Accuracy: 89.9023\n",
      "Epoch [53/1000], Train Loss: 1.0874, Train Accuracy: 88.1524, Test Loss: 1.0701, Test Accuracy: 89.9023\n",
      "Epoch [54/1000], Train Loss: 1.0872, Train Accuracy: 88.2353, Test Loss: 1.0699, Test Accuracy: 89.9023\n",
      "Epoch [55/1000], Train Loss: 1.0871, Train Accuracy: 88.2353, Test Loss: 1.0697, Test Accuracy: 89.9023\n",
      "Epoch [56/1000], Train Loss: 1.0869, Train Accuracy: 88.2353, Test Loss: 1.0695, Test Accuracy: 89.9023\n",
      "Epoch [57/1000], Train Loss: 1.0868, Train Accuracy: 88.2353, Test Loss: 1.0694, Test Accuracy: 89.9023\n",
      "Epoch [58/1000], Train Loss: 1.0866, Train Accuracy: 88.2353, Test Loss: 1.0692, Test Accuracy: 89.9023\n",
      "Epoch [59/1000], Train Loss: 1.0865, Train Accuracy: 88.3181, Test Loss: 1.0690, Test Accuracy: 89.9023\n",
      "Epoch [60/1000], Train Loss: 1.0863, Train Accuracy: 88.4010, Test Loss: 1.0688, Test Accuracy: 89.9023\n",
      "Epoch [61/1000], Train Loss: 1.0862, Train Accuracy: 88.4838, Test Loss: 1.0687, Test Accuracy: 89.9023\n",
      "Epoch [62/1000], Train Loss: 1.0860, Train Accuracy: 88.4838, Test Loss: 1.0685, Test Accuracy: 89.9023\n",
      "Epoch [63/1000], Train Loss: 1.0859, Train Accuracy: 88.4838, Test Loss: 1.0683, Test Accuracy: 89.9023\n",
      "Epoch [64/1000], Train Loss: 1.0857, Train Accuracy: 88.4838, Test Loss: 1.0681, Test Accuracy: 89.9023\n",
      "Epoch [65/1000], Train Loss: 1.0856, Train Accuracy: 88.4838, Test Loss: 1.0680, Test Accuracy: 89.9023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/1000], Train Loss: 1.0854, Train Accuracy: 88.4838, Test Loss: 1.0678, Test Accuracy: 89.9023\n",
      "Epoch [67/1000], Train Loss: 1.0853, Train Accuracy: 88.4838, Test Loss: 1.0676, Test Accuracy: 89.9023\n",
      "Epoch [68/1000], Train Loss: 1.0851, Train Accuracy: 88.4838, Test Loss: 1.0674, Test Accuracy: 89.9023\n",
      "Epoch [69/1000], Train Loss: 1.0850, Train Accuracy: 88.4838, Test Loss: 1.0673, Test Accuracy: 89.9023\n",
      "Epoch [70/1000], Train Loss: 1.0848, Train Accuracy: 88.4838, Test Loss: 1.0671, Test Accuracy: 89.9023\n",
      "Epoch [71/1000], Train Loss: 1.0847, Train Accuracy: 88.4838, Test Loss: 1.0669, Test Accuracy: 89.9023\n",
      "Epoch [72/1000], Train Loss: 1.0845, Train Accuracy: 88.4838, Test Loss: 1.0667, Test Accuracy: 89.9023\n",
      "Epoch [73/1000], Train Loss: 1.0844, Train Accuracy: 88.4838, Test Loss: 1.0666, Test Accuracy: 89.9023\n",
      "Epoch [74/1000], Train Loss: 1.0842, Train Accuracy: 88.4838, Test Loss: 1.0664, Test Accuracy: 89.9023\n",
      "Epoch [75/1000], Train Loss: 1.0841, Train Accuracy: 88.4838, Test Loss: 1.0662, Test Accuracy: 89.9023\n",
      "Epoch [76/1000], Train Loss: 1.0839, Train Accuracy: 88.4838, Test Loss: 1.0660, Test Accuracy: 89.9023\n",
      "Epoch [77/1000], Train Loss: 1.0838, Train Accuracy: 88.4838, Test Loss: 1.0658, Test Accuracy: 89.9023\n",
      "Epoch [78/1000], Train Loss: 1.0836, Train Accuracy: 88.4838, Test Loss: 1.0657, Test Accuracy: 89.9023\n",
      "Epoch [79/1000], Train Loss: 1.0835, Train Accuracy: 88.4838, Test Loss: 1.0655, Test Accuracy: 89.9023\n",
      "Epoch [80/1000], Train Loss: 1.0833, Train Accuracy: 88.4838, Test Loss: 1.0653, Test Accuracy: 89.9023\n",
      "Epoch [81/1000], Train Loss: 1.0832, Train Accuracy: 88.4838, Test Loss: 1.0651, Test Accuracy: 89.9023\n",
      "Epoch [82/1000], Train Loss: 1.0830, Train Accuracy: 88.4838, Test Loss: 1.0650, Test Accuracy: 89.9023\n",
      "Epoch [83/1000], Train Loss: 1.0829, Train Accuracy: 88.4838, Test Loss: 1.0648, Test Accuracy: 89.9023\n",
      "Epoch [84/1000], Train Loss: 1.0827, Train Accuracy: 88.4838, Test Loss: 1.0646, Test Accuracy: 89.9023\n",
      "Epoch [85/1000], Train Loss: 1.0826, Train Accuracy: 88.4838, Test Loss: 1.0644, Test Accuracy: 89.9023\n",
      "Epoch [86/1000], Train Loss: 1.0824, Train Accuracy: 88.4838, Test Loss: 1.0642, Test Accuracy: 89.9023\n",
      "Epoch [87/1000], Train Loss: 1.0823, Train Accuracy: 88.4838, Test Loss: 1.0641, Test Accuracy: 89.9023\n",
      "Epoch [88/1000], Train Loss: 1.0821, Train Accuracy: 88.4838, Test Loss: 1.0639, Test Accuracy: 89.9023\n",
      "Epoch [89/1000], Train Loss: 1.0820, Train Accuracy: 88.4838, Test Loss: 1.0637, Test Accuracy: 89.9023\n",
      "Epoch [90/1000], Train Loss: 1.0818, Train Accuracy: 88.4838, Test Loss: 1.0635, Test Accuracy: 89.9023\n",
      "Epoch [91/1000], Train Loss: 1.0816, Train Accuracy: 88.4838, Test Loss: 1.0633, Test Accuracy: 89.9023\n",
      "Epoch [92/1000], Train Loss: 1.0815, Train Accuracy: 88.4838, Test Loss: 1.0631, Test Accuracy: 89.9023\n",
      "Epoch [93/1000], Train Loss: 1.0813, Train Accuracy: 88.4838, Test Loss: 1.0629, Test Accuracy: 89.9023\n",
      "Epoch [94/1000], Train Loss: 1.0812, Train Accuracy: 88.4838, Test Loss: 1.0628, Test Accuracy: 89.9023\n",
      "Epoch [95/1000], Train Loss: 1.0810, Train Accuracy: 88.4838, Test Loss: 1.0626, Test Accuracy: 89.9023\n",
      "Epoch [96/1000], Train Loss: 1.0809, Train Accuracy: 88.4838, Test Loss: 1.0624, Test Accuracy: 89.9023\n",
      "Epoch [97/1000], Train Loss: 1.0807, Train Accuracy: 88.4838, Test Loss: 1.0622, Test Accuracy: 89.9023\n",
      "Epoch [98/1000], Train Loss: 1.0805, Train Accuracy: 88.4838, Test Loss: 1.0620, Test Accuracy: 89.9023\n",
      "Epoch [99/1000], Train Loss: 1.0804, Train Accuracy: 88.4838, Test Loss: 1.0618, Test Accuracy: 89.9023\n",
      "Epoch [100/1000], Train Loss: 1.0802, Train Accuracy: 88.4838, Test Loss: 1.0616, Test Accuracy: 89.9023\n",
      "Epoch [101/1000], Train Loss: 1.0800, Train Accuracy: 88.4838, Test Loss: 1.0614, Test Accuracy: 89.9023\n",
      "Epoch [102/1000], Train Loss: 1.0799, Train Accuracy: 88.4838, Test Loss: 1.0612, Test Accuracy: 89.9023\n",
      "Epoch [103/1000], Train Loss: 1.0797, Train Accuracy: 88.4838, Test Loss: 1.0611, Test Accuracy: 89.9023\n",
      "Epoch [104/1000], Train Loss: 1.0796, Train Accuracy: 88.4838, Test Loss: 1.0609, Test Accuracy: 89.9023\n",
      "Epoch [105/1000], Train Loss: 1.0794, Train Accuracy: 88.4838, Test Loss: 1.0607, Test Accuracy: 89.9023\n",
      "Epoch [106/1000], Train Loss: 1.0792, Train Accuracy: 88.4838, Test Loss: 1.0605, Test Accuracy: 89.9023\n",
      "Epoch [107/1000], Train Loss: 1.0791, Train Accuracy: 88.4838, Test Loss: 1.0603, Test Accuracy: 89.9023\n",
      "Epoch [108/1000], Train Loss: 1.0789, Train Accuracy: 88.4838, Test Loss: 1.0601, Test Accuracy: 89.9023\n",
      "Epoch [109/1000], Train Loss: 1.0787, Train Accuracy: 88.4838, Test Loss: 1.0599, Test Accuracy: 89.9023\n",
      "Epoch [110/1000], Train Loss: 1.0786, Train Accuracy: 88.4838, Test Loss: 1.0597, Test Accuracy: 89.9023\n",
      "Epoch [111/1000], Train Loss: 1.0784, Train Accuracy: 88.4838, Test Loss: 1.0595, Test Accuracy: 89.9023\n",
      "Epoch [112/1000], Train Loss: 1.0782, Train Accuracy: 88.4838, Test Loss: 1.0593, Test Accuracy: 89.9023\n",
      "Epoch [113/1000], Train Loss: 1.0781, Train Accuracy: 88.4838, Test Loss: 1.0591, Test Accuracy: 89.9023\n",
      "Epoch [114/1000], Train Loss: 1.0779, Train Accuracy: 88.4838, Test Loss: 1.0589, Test Accuracy: 89.9023\n",
      "Epoch [115/1000], Train Loss: 1.0777, Train Accuracy: 88.4838, Test Loss: 1.0587, Test Accuracy: 89.9023\n",
      "Epoch [116/1000], Train Loss: 1.0776, Train Accuracy: 88.4838, Test Loss: 1.0586, Test Accuracy: 89.9023\n",
      "Epoch [117/1000], Train Loss: 1.0774, Train Accuracy: 88.4838, Test Loss: 1.0584, Test Accuracy: 89.9023\n",
      "Epoch [118/1000], Train Loss: 1.0772, Train Accuracy: 88.4838, Test Loss: 1.0582, Test Accuracy: 89.9023\n",
      "Epoch [119/1000], Train Loss: 1.0771, Train Accuracy: 88.4838, Test Loss: 1.0580, Test Accuracy: 89.9023\n",
      "Epoch [120/1000], Train Loss: 1.0769, Train Accuracy: 88.4838, Test Loss: 1.0578, Test Accuracy: 89.9023\n",
      "Epoch [121/1000], Train Loss: 1.0767, Train Accuracy: 88.4838, Test Loss: 1.0576, Test Accuracy: 89.9023\n",
      "Epoch [122/1000], Train Loss: 1.0766, Train Accuracy: 88.4838, Test Loss: 1.0574, Test Accuracy: 89.9023\n",
      "Epoch [123/1000], Train Loss: 1.0764, Train Accuracy: 88.4838, Test Loss: 1.0572, Test Accuracy: 89.9023\n",
      "Epoch [124/1000], Train Loss: 1.0762, Train Accuracy: 88.4838, Test Loss: 1.0570, Test Accuracy: 89.9023\n",
      "Epoch [125/1000], Train Loss: 1.0760, Train Accuracy: 88.4838, Test Loss: 1.0568, Test Accuracy: 89.9023\n",
      "Epoch [126/1000], Train Loss: 1.0759, Train Accuracy: 88.4838, Test Loss: 1.0566, Test Accuracy: 89.9023\n",
      "Epoch [127/1000], Train Loss: 1.0757, Train Accuracy: 88.4838, Test Loss: 1.0564, Test Accuracy: 89.9023\n",
      "Epoch [128/1000], Train Loss: 1.0755, Train Accuracy: 88.4838, Test Loss: 1.0562, Test Accuracy: 89.9023\n",
      "Epoch [129/1000], Train Loss: 1.0753, Train Accuracy: 88.4838, Test Loss: 1.0560, Test Accuracy: 89.9023\n",
      "Epoch [130/1000], Train Loss: 1.0752, Train Accuracy: 88.4838, Test Loss: 1.0558, Test Accuracy: 89.9023\n",
      "Epoch [131/1000], Train Loss: 1.0750, Train Accuracy: 88.4838, Test Loss: 1.0556, Test Accuracy: 89.9023\n",
      "Epoch [132/1000], Train Loss: 1.0748, Train Accuracy: 88.4838, Test Loss: 1.0554, Test Accuracy: 89.9023\n",
      "Epoch [133/1000], Train Loss: 1.0747, Train Accuracy: 88.4838, Test Loss: 1.0552, Test Accuracy: 89.9023\n",
      "Epoch [134/1000], Train Loss: 1.0745, Train Accuracy: 88.4838, Test Loss: 1.0550, Test Accuracy: 89.9023\n",
      "Epoch [135/1000], Train Loss: 1.0743, Train Accuracy: 88.4838, Test Loss: 1.0548, Test Accuracy: 89.9023\n",
      "Epoch [136/1000], Train Loss: 1.0741, Train Accuracy: 88.4838, Test Loss: 1.0546, Test Accuracy: 89.9023\n",
      "Epoch [137/1000], Train Loss: 1.0739, Train Accuracy: 88.4838, Test Loss: 1.0544, Test Accuracy: 89.9023\n",
      "Epoch [138/1000], Train Loss: 1.0738, Train Accuracy: 88.4838, Test Loss: 1.0542, Test Accuracy: 89.9023\n",
      "Epoch [139/1000], Train Loss: 1.0736, Train Accuracy: 88.4838, Test Loss: 1.0540, Test Accuracy: 89.9023\n",
      "Epoch [140/1000], Train Loss: 1.0734, Train Accuracy: 88.4838, Test Loss: 1.0538, Test Accuracy: 89.9023\n",
      "Epoch [141/1000], Train Loss: 1.0732, Train Accuracy: 88.4838, Test Loss: 1.0536, Test Accuracy: 89.9023\n",
      "Epoch [142/1000], Train Loss: 1.0730, Train Accuracy: 88.4838, Test Loss: 1.0533, Test Accuracy: 89.9023\n",
      "Epoch [143/1000], Train Loss: 1.0729, Train Accuracy: 88.4838, Test Loss: 1.0531, Test Accuracy: 89.9023\n",
      "Epoch [144/1000], Train Loss: 1.0727, Train Accuracy: 88.4838, Test Loss: 1.0529, Test Accuracy: 89.9023\n",
      "Epoch [145/1000], Train Loss: 1.0725, Train Accuracy: 88.4838, Test Loss: 1.0527, Test Accuracy: 89.9023\n",
      "Epoch [146/1000], Train Loss: 1.0723, Train Accuracy: 88.4838, Test Loss: 1.0525, Test Accuracy: 89.9023\n",
      "Epoch [147/1000], Train Loss: 1.0721, Train Accuracy: 88.4838, Test Loss: 1.0523, Test Accuracy: 89.9023\n",
      "Epoch [148/1000], Train Loss: 1.0720, Train Accuracy: 88.4838, Test Loss: 1.0521, Test Accuracy: 89.9023\n",
      "Epoch [149/1000], Train Loss: 1.0718, Train Accuracy: 88.4838, Test Loss: 1.0519, Test Accuracy: 89.9023\n",
      "Epoch [150/1000], Train Loss: 1.0716, Train Accuracy: 88.4838, Test Loss: 1.0517, Test Accuracy: 89.9023\n",
      "Epoch [151/1000], Train Loss: 1.0714, Train Accuracy: 88.4838, Test Loss: 1.0515, Test Accuracy: 89.9023\n",
      "Epoch [152/1000], Train Loss: 1.0712, Train Accuracy: 88.4838, Test Loss: 1.0513, Test Accuracy: 89.9023\n",
      "Epoch [153/1000], Train Loss: 1.0710, Train Accuracy: 88.4838, Test Loss: 1.0510, Test Accuracy: 89.9023\n",
      "Epoch [154/1000], Train Loss: 1.0709, Train Accuracy: 88.4838, Test Loss: 1.0508, Test Accuracy: 89.9023\n",
      "Epoch [155/1000], Train Loss: 1.0707, Train Accuracy: 88.4838, Test Loss: 1.0506, Test Accuracy: 89.9023\n",
      "Epoch [156/1000], Train Loss: 1.0705, Train Accuracy: 88.4838, Test Loss: 1.0504, Test Accuracy: 89.9023\n",
      "Epoch [157/1000], Train Loss: 1.0703, Train Accuracy: 88.4838, Test Loss: 1.0502, Test Accuracy: 89.9023\n",
      "Epoch [158/1000], Train Loss: 1.0701, Train Accuracy: 88.4838, Test Loss: 1.0500, Test Accuracy: 89.9023\n",
      "Epoch [159/1000], Train Loss: 1.0699, Train Accuracy: 88.4838, Test Loss: 1.0498, Test Accuracy: 89.9023\n",
      "Epoch [160/1000], Train Loss: 1.0697, Train Accuracy: 88.4838, Test Loss: 1.0496, Test Accuracy: 89.9023\n",
      "Epoch [161/1000], Train Loss: 1.0696, Train Accuracy: 88.4838, Test Loss: 1.0493, Test Accuracy: 89.9023\n",
      "Epoch [162/1000], Train Loss: 1.0694, Train Accuracy: 88.4838, Test Loss: 1.0491, Test Accuracy: 89.9023\n",
      "Epoch [163/1000], Train Loss: 1.0692, Train Accuracy: 88.4838, Test Loss: 1.0489, Test Accuracy: 89.9023\n",
      "Epoch [164/1000], Train Loss: 1.0690, Train Accuracy: 88.4838, Test Loss: 1.0487, Test Accuracy: 89.9023\n",
      "Epoch [165/1000], Train Loss: 1.0688, Train Accuracy: 88.4838, Test Loss: 1.0485, Test Accuracy: 89.9023\n",
      "Epoch [166/1000], Train Loss: 1.0686, Train Accuracy: 88.4838, Test Loss: 1.0483, Test Accuracy: 89.9023\n",
      "Epoch [167/1000], Train Loss: 1.0684, Train Accuracy: 88.4838, Test Loss: 1.0481, Test Accuracy: 89.9023\n",
      "Epoch [168/1000], Train Loss: 1.0682, Train Accuracy: 88.4838, Test Loss: 1.0478, Test Accuracy: 89.9023\n",
      "Epoch [169/1000], Train Loss: 1.0680, Train Accuracy: 88.4838, Test Loss: 1.0476, Test Accuracy: 89.9023\n",
      "Epoch [170/1000], Train Loss: 1.0679, Train Accuracy: 88.4838, Test Loss: 1.0474, Test Accuracy: 89.9023\n",
      "Epoch [171/1000], Train Loss: 1.0677, Train Accuracy: 88.4838, Test Loss: 1.0472, Test Accuracy: 89.9023\n",
      "Epoch [172/1000], Train Loss: 1.0675, Train Accuracy: 88.4838, Test Loss: 1.0470, Test Accuracy: 89.9023\n",
      "Epoch [173/1000], Train Loss: 1.0673, Train Accuracy: 88.4838, Test Loss: 1.0468, Test Accuracy: 89.9023\n",
      "Epoch [174/1000], Train Loss: 1.0671, Train Accuracy: 88.4838, Test Loss: 1.0466, Test Accuracy: 89.9023\n",
      "Epoch [175/1000], Train Loss: 1.0669, Train Accuracy: 88.4838, Test Loss: 1.0463, Test Accuracy: 89.9023\n",
      "Epoch [176/1000], Train Loss: 1.0667, Train Accuracy: 88.4838, Test Loss: 1.0461, Test Accuracy: 89.9023\n",
      "Epoch [177/1000], Train Loss: 1.0665, Train Accuracy: 88.4838, Test Loss: 1.0459, Test Accuracy: 89.9023\n",
      "Epoch [178/1000], Train Loss: 1.0663, Train Accuracy: 88.4838, Test Loss: 1.0457, Test Accuracy: 89.9023\n",
      "Epoch [179/1000], Train Loss: 1.0661, Train Accuracy: 88.4838, Test Loss: 1.0455, Test Accuracy: 89.9023\n",
      "Epoch [180/1000], Train Loss: 1.0660, Train Accuracy: 88.4838, Test Loss: 1.0453, Test Accuracy: 89.9023\n",
      "Epoch [181/1000], Train Loss: 1.0658, Train Accuracy: 88.4838, Test Loss: 1.0450, Test Accuracy: 89.9023\n",
      "Epoch [182/1000], Train Loss: 1.0656, Train Accuracy: 88.4838, Test Loss: 1.0448, Test Accuracy: 89.9023\n",
      "Epoch [183/1000], Train Loss: 1.0654, Train Accuracy: 88.4838, Test Loss: 1.0446, Test Accuracy: 89.9023\n",
      "Epoch [184/1000], Train Loss: 1.0652, Train Accuracy: 88.4838, Test Loss: 1.0444, Test Accuracy: 89.9023\n",
      "Epoch [185/1000], Train Loss: 1.0650, Train Accuracy: 88.4838, Test Loss: 1.0442, Test Accuracy: 89.9023\n",
      "Epoch [186/1000], Train Loss: 1.0648, Train Accuracy: 88.4838, Test Loss: 1.0440, Test Accuracy: 89.9023\n",
      "Epoch [187/1000], Train Loss: 1.0646, Train Accuracy: 88.4838, Test Loss: 1.0437, Test Accuracy: 89.9023\n",
      "Epoch [188/1000], Train Loss: 1.0644, Train Accuracy: 88.4838, Test Loss: 1.0435, Test Accuracy: 89.9023\n",
      "Epoch [189/1000], Train Loss: 1.0642, Train Accuracy: 88.4838, Test Loss: 1.0433, Test Accuracy: 89.9023\n",
      "Epoch [190/1000], Train Loss: 1.0640, Train Accuracy: 88.4838, Test Loss: 1.0431, Test Accuracy: 89.9023\n",
      "Epoch [191/1000], Train Loss: 1.0638, Train Accuracy: 88.4838, Test Loss: 1.0429, Test Accuracy: 89.9023\n",
      "Epoch [192/1000], Train Loss: 1.0636, Train Accuracy: 88.4838, Test Loss: 1.0426, Test Accuracy: 89.9023\n",
      "Epoch [193/1000], Train Loss: 1.0634, Train Accuracy: 88.4838, Test Loss: 1.0424, Test Accuracy: 89.9023\n",
      "Epoch [194/1000], Train Loss: 1.0632, Train Accuracy: 88.4838, Test Loss: 1.0422, Test Accuracy: 89.9023\n",
      "Epoch [195/1000], Train Loss: 1.0631, Train Accuracy: 88.4838, Test Loss: 1.0420, Test Accuracy: 89.9023\n",
      "Epoch [196/1000], Train Loss: 1.0629, Train Accuracy: 88.4838, Test Loss: 1.0418, Test Accuracy: 89.9023\n",
      "Epoch [197/1000], Train Loss: 1.0627, Train Accuracy: 88.4838, Test Loss: 1.0416, Test Accuracy: 89.9023\n",
      "Epoch [198/1000], Train Loss: 1.0625, Train Accuracy: 88.4838, Test Loss: 1.0413, Test Accuracy: 89.9023\n",
      "Epoch [199/1000], Train Loss: 1.0623, Train Accuracy: 88.4838, Test Loss: 1.0411, Test Accuracy: 89.9023\n",
      "Epoch [200/1000], Train Loss: 1.0621, Train Accuracy: 88.4838, Test Loss: 1.0409, Test Accuracy: 89.9023\n",
      "Epoch [201/1000], Train Loss: 1.0619, Train Accuracy: 88.4838, Test Loss: 1.0407, Test Accuracy: 89.9023\n",
      "Epoch [202/1000], Train Loss: 1.0617, Train Accuracy: 88.4838, Test Loss: 1.0405, Test Accuracy: 89.9023\n",
      "Epoch [203/1000], Train Loss: 1.0615, Train Accuracy: 88.4838, Test Loss: 1.0402, Test Accuracy: 89.9023\n",
      "Epoch [204/1000], Train Loss: 1.0613, Train Accuracy: 88.4838, Test Loss: 1.0400, Test Accuracy: 89.9023\n",
      "Epoch [205/1000], Train Loss: 1.0611, Train Accuracy: 88.4838, Test Loss: 1.0398, Test Accuracy: 89.9023\n",
      "Epoch [206/1000], Train Loss: 1.0609, Train Accuracy: 88.4838, Test Loss: 1.0396, Test Accuracy: 89.9023\n",
      "Epoch [207/1000], Train Loss: 1.0607, Train Accuracy: 88.4838, Test Loss: 1.0394, Test Accuracy: 89.9023\n",
      "Epoch [208/1000], Train Loss: 1.0605, Train Accuracy: 88.4838, Test Loss: 1.0391, Test Accuracy: 89.9023\n",
      "Epoch [209/1000], Train Loss: 1.0603, Train Accuracy: 88.4838, Test Loss: 1.0389, Test Accuracy: 89.9023\n",
      "Epoch [210/1000], Train Loss: 1.0601, Train Accuracy: 88.4838, Test Loss: 1.0387, Test Accuracy: 89.9023\n",
      "Epoch [211/1000], Train Loss: 1.0599, Train Accuracy: 88.4838, Test Loss: 1.0385, Test Accuracy: 89.9023\n",
      "Epoch [212/1000], Train Loss: 1.0597, Train Accuracy: 88.4838, Test Loss: 1.0383, Test Accuracy: 89.9023\n",
      "Epoch [213/1000], Train Loss: 1.0595, Train Accuracy: 88.4838, Test Loss: 1.0381, Test Accuracy: 89.9023\n",
      "Epoch [214/1000], Train Loss: 1.0593, Train Accuracy: 88.4838, Test Loss: 1.0378, Test Accuracy: 89.9023\n",
      "Epoch [215/1000], Train Loss: 1.0591, Train Accuracy: 88.4838, Test Loss: 1.0376, Test Accuracy: 89.9023\n",
      "Epoch [216/1000], Train Loss: 1.0589, Train Accuracy: 88.4838, Test Loss: 1.0374, Test Accuracy: 89.9023\n",
      "Epoch [217/1000], Train Loss: 1.0587, Train Accuracy: 88.4838, Test Loss: 1.0372, Test Accuracy: 89.9023\n",
      "Epoch [218/1000], Train Loss: 1.0585, Train Accuracy: 88.4838, Test Loss: 1.0370, Test Accuracy: 89.9023\n",
      "Epoch [219/1000], Train Loss: 1.0583, Train Accuracy: 88.4838, Test Loss: 1.0367, Test Accuracy: 89.9023\n",
      "Epoch [220/1000], Train Loss: 1.0581, Train Accuracy: 88.4838, Test Loss: 1.0365, Test Accuracy: 89.9023\n",
      "Epoch [221/1000], Train Loss: 1.0579, Train Accuracy: 88.4838, Test Loss: 1.0363, Test Accuracy: 89.9023\n",
      "Epoch [222/1000], Train Loss: 1.0577, Train Accuracy: 88.4838, Test Loss: 1.0361, Test Accuracy: 89.9023\n",
      "Epoch [223/1000], Train Loss: 1.0575, Train Accuracy: 88.4838, Test Loss: 1.0359, Test Accuracy: 89.9023\n",
      "Epoch [224/1000], Train Loss: 1.0573, Train Accuracy: 88.4838, Test Loss: 1.0356, Test Accuracy: 89.9023\n",
      "Epoch [225/1000], Train Loss: 1.0571, Train Accuracy: 88.4838, Test Loss: 1.0354, Test Accuracy: 89.9023\n",
      "Epoch [226/1000], Train Loss: 1.0569, Train Accuracy: 88.4838, Test Loss: 1.0352, Test Accuracy: 89.9023\n",
      "Epoch [227/1000], Train Loss: 1.0567, Train Accuracy: 88.4838, Test Loss: 1.0350, Test Accuracy: 89.9023\n",
      "Epoch [228/1000], Train Loss: 1.0565, Train Accuracy: 88.4838, Test Loss: 1.0348, Test Accuracy: 89.9023\n",
      "Epoch [229/1000], Train Loss: 1.0563, Train Accuracy: 88.4838, Test Loss: 1.0346, Test Accuracy: 89.9023\n",
      "Epoch [230/1000], Train Loss: 1.0561, Train Accuracy: 88.4838, Test Loss: 1.0343, Test Accuracy: 89.9023\n",
      "Epoch [231/1000], Train Loss: 1.0559, Train Accuracy: 88.4838, Test Loss: 1.0341, Test Accuracy: 89.9023\n",
      "Epoch [232/1000], Train Loss: 1.0557, Train Accuracy: 88.4838, Test Loss: 1.0339, Test Accuracy: 89.9023\n",
      "Epoch [233/1000], Train Loss: 1.0555, Train Accuracy: 88.4838, Test Loss: 1.0337, Test Accuracy: 89.9023\n",
      "Epoch [234/1000], Train Loss: 1.0553, Train Accuracy: 88.4838, Test Loss: 1.0335, Test Accuracy: 89.9023\n",
      "Epoch [235/1000], Train Loss: 1.0551, Train Accuracy: 88.4838, Test Loss: 1.0333, Test Accuracy: 89.9023\n",
      "Epoch [236/1000], Train Loss: 1.0549, Train Accuracy: 88.4838, Test Loss: 1.0330, Test Accuracy: 89.9023\n",
      "Epoch [237/1000], Train Loss: 1.0547, Train Accuracy: 88.4838, Test Loss: 1.0328, Test Accuracy: 89.9023\n",
      "Epoch [238/1000], Train Loss: 1.0545, Train Accuracy: 88.4838, Test Loss: 1.0326, Test Accuracy: 89.9023\n",
      "Epoch [239/1000], Train Loss: 1.0543, Train Accuracy: 88.4838, Test Loss: 1.0324, Test Accuracy: 89.9023\n",
      "Epoch [240/1000], Train Loss: 1.0541, Train Accuracy: 88.4838, Test Loss: 1.0322, Test Accuracy: 89.9023\n",
      "Epoch [241/1000], Train Loss: 1.0539, Train Accuracy: 88.4838, Test Loss: 1.0319, Test Accuracy: 89.9023\n",
      "Epoch [242/1000], Train Loss: 1.0537, Train Accuracy: 88.4838, Test Loss: 1.0317, Test Accuracy: 89.9023\n",
      "Epoch [243/1000], Train Loss: 1.0535, Train Accuracy: 88.4838, Test Loss: 1.0315, Test Accuracy: 89.9023\n",
      "Epoch [244/1000], Train Loss: 1.0533, Train Accuracy: 88.4838, Test Loss: 1.0313, Test Accuracy: 89.9023\n",
      "Epoch [245/1000], Train Loss: 1.0530, Train Accuracy: 88.4838, Test Loss: 1.0311, Test Accuracy: 89.9023\n",
      "Epoch [246/1000], Train Loss: 1.0528, Train Accuracy: 88.4838, Test Loss: 1.0309, Test Accuracy: 89.9023\n",
      "Epoch [247/1000], Train Loss: 1.0526, Train Accuracy: 88.4838, Test Loss: 1.0306, Test Accuracy: 89.9023\n",
      "Epoch [248/1000], Train Loss: 1.0524, Train Accuracy: 88.4838, Test Loss: 1.0304, Test Accuracy: 89.9023\n",
      "Epoch [249/1000], Train Loss: 1.0522, Train Accuracy: 88.4838, Test Loss: 1.0302, Test Accuracy: 89.9023\n",
      "Epoch [250/1000], Train Loss: 1.0520, Train Accuracy: 88.4838, Test Loss: 1.0300, Test Accuracy: 89.9023\n",
      "Epoch [251/1000], Train Loss: 1.0518, Train Accuracy: 88.4838, Test Loss: 1.0297, Test Accuracy: 89.9023\n",
      "Epoch [252/1000], Train Loss: 1.0516, Train Accuracy: 88.4838, Test Loss: 1.0295, Test Accuracy: 89.9023\n",
      "Epoch [253/1000], Train Loss: 1.0514, Train Accuracy: 88.4838, Test Loss: 1.0293, Test Accuracy: 89.9023\n",
      "Epoch [254/1000], Train Loss: 1.0512, Train Accuracy: 88.4838, Test Loss: 1.0291, Test Accuracy: 89.9023\n",
      "Epoch [255/1000], Train Loss: 1.0510, Train Accuracy: 88.4838, Test Loss: 1.0289, Test Accuracy: 89.9023\n",
      "Epoch [256/1000], Train Loss: 1.0507, Train Accuracy: 88.4838, Test Loss: 1.0286, Test Accuracy: 89.9023\n",
      "Epoch [257/1000], Train Loss: 1.0505, Train Accuracy: 88.4838, Test Loss: 1.0284, Test Accuracy: 89.9023\n",
      "Epoch [258/1000], Train Loss: 1.0503, Train Accuracy: 88.4838, Test Loss: 1.0282, Test Accuracy: 89.9023\n",
      "Epoch [259/1000], Train Loss: 1.0501, Train Accuracy: 88.4838, Test Loss: 1.0280, Test Accuracy: 89.9023\n",
      "Epoch [260/1000], Train Loss: 1.0499, Train Accuracy: 88.4838, Test Loss: 1.0277, Test Accuracy: 89.9023\n",
      "Epoch [261/1000], Train Loss: 1.0497, Train Accuracy: 88.4838, Test Loss: 1.0275, Test Accuracy: 89.9023\n",
      "Epoch [262/1000], Train Loss: 1.0495, Train Accuracy: 88.4838, Test Loss: 1.0273, Test Accuracy: 89.9023\n",
      "Epoch [263/1000], Train Loss: 1.0493, Train Accuracy: 88.4838, Test Loss: 1.0271, Test Accuracy: 89.9023\n",
      "Epoch [264/1000], Train Loss: 1.0490, Train Accuracy: 88.4838, Test Loss: 1.0268, Test Accuracy: 89.9023\n",
      "Epoch [265/1000], Train Loss: 1.0488, Train Accuracy: 88.4838, Test Loss: 1.0266, Test Accuracy: 89.9023\n",
      "Epoch [266/1000], Train Loss: 1.0486, Train Accuracy: 88.4838, Test Loss: 1.0264, Test Accuracy: 89.9023\n",
      "Epoch [267/1000], Train Loss: 1.0484, Train Accuracy: 88.4838, Test Loss: 1.0262, Test Accuracy: 89.9023\n",
      "Epoch [268/1000], Train Loss: 1.0482, Train Accuracy: 88.4838, Test Loss: 1.0259, Test Accuracy: 89.9023\n",
      "Epoch [269/1000], Train Loss: 1.0480, Train Accuracy: 88.4838, Test Loss: 1.0257, Test Accuracy: 89.9023\n",
      "Epoch [270/1000], Train Loss: 1.0477, Train Accuracy: 88.4838, Test Loss: 1.0255, Test Accuracy: 89.9023\n",
      "Epoch [271/1000], Train Loss: 1.0475, Train Accuracy: 88.4838, Test Loss: 1.0253, Test Accuracy: 89.9023\n",
      "Epoch [272/1000], Train Loss: 1.0473, Train Accuracy: 88.4838, Test Loss: 1.0250, Test Accuracy: 89.9023\n",
      "Epoch [273/1000], Train Loss: 1.0471, Train Accuracy: 88.4838, Test Loss: 1.0248, Test Accuracy: 89.9023\n",
      "Epoch [274/1000], Train Loss: 1.0469, Train Accuracy: 88.4838, Test Loss: 1.0246, Test Accuracy: 89.9023\n",
      "Epoch [275/1000], Train Loss: 1.0467, Train Accuracy: 88.4838, Test Loss: 1.0244, Test Accuracy: 89.9023\n",
      "Epoch [276/1000], Train Loss: 1.0464, Train Accuracy: 88.4838, Test Loss: 1.0241, Test Accuracy: 89.9023\n",
      "Epoch [277/1000], Train Loss: 1.0462, Train Accuracy: 88.4838, Test Loss: 1.0239, Test Accuracy: 89.9023\n",
      "Epoch [278/1000], Train Loss: 1.0460, Train Accuracy: 88.4838, Test Loss: 1.0237, Test Accuracy: 89.9023\n",
      "Epoch [279/1000], Train Loss: 1.0458, Train Accuracy: 88.4838, Test Loss: 1.0235, Test Accuracy: 89.9023\n",
      "Epoch [280/1000], Train Loss: 1.0456, Train Accuracy: 88.4838, Test Loss: 1.0232, Test Accuracy: 89.9023\n",
      "Epoch [281/1000], Train Loss: 1.0453, Train Accuracy: 88.4838, Test Loss: 1.0230, Test Accuracy: 89.9023\n",
      "Epoch [282/1000], Train Loss: 1.0451, Train Accuracy: 88.4838, Test Loss: 1.0228, Test Accuracy: 89.9023\n",
      "Epoch [283/1000], Train Loss: 1.0449, Train Accuracy: 88.4838, Test Loss: 1.0226, Test Accuracy: 89.9023\n",
      "Epoch [284/1000], Train Loss: 1.0447, Train Accuracy: 88.4838, Test Loss: 1.0223, Test Accuracy: 89.9023\n",
      "Epoch [285/1000], Train Loss: 1.0445, Train Accuracy: 88.4838, Test Loss: 1.0221, Test Accuracy: 89.9023\n",
      "Epoch [286/1000], Train Loss: 1.0442, Train Accuracy: 88.4838, Test Loss: 1.0219, Test Accuracy: 89.9023\n",
      "Epoch [287/1000], Train Loss: 1.0440, Train Accuracy: 88.4838, Test Loss: 1.0217, Test Accuracy: 89.9023\n",
      "Epoch [288/1000], Train Loss: 1.0438, Train Accuracy: 88.4838, Test Loss: 1.0214, Test Accuracy: 89.9023\n",
      "Epoch [289/1000], Train Loss: 1.0436, Train Accuracy: 88.4838, Test Loss: 1.0212, Test Accuracy: 89.9023\n",
      "Epoch [290/1000], Train Loss: 1.0434, Train Accuracy: 88.4838, Test Loss: 1.0210, Test Accuracy: 89.9023\n",
      "Epoch [291/1000], Train Loss: 1.0431, Train Accuracy: 88.4838, Test Loss: 1.0208, Test Accuracy: 89.9023\n",
      "Epoch [292/1000], Train Loss: 1.0429, Train Accuracy: 88.4838, Test Loss: 1.0205, Test Accuracy: 89.9023\n",
      "Epoch [293/1000], Train Loss: 1.0427, Train Accuracy: 88.4838, Test Loss: 1.0203, Test Accuracy: 89.9023\n",
      "Epoch [294/1000], Train Loss: 1.0425, Train Accuracy: 88.4838, Test Loss: 1.0201, Test Accuracy: 89.9023\n",
      "Epoch [295/1000], Train Loss: 1.0423, Train Accuracy: 88.4838, Test Loss: 1.0199, Test Accuracy: 89.9023\n",
      "Epoch [296/1000], Train Loss: 1.0420, Train Accuracy: 88.4838, Test Loss: 1.0196, Test Accuracy: 89.9023\n",
      "Epoch [297/1000], Train Loss: 1.0418, Train Accuracy: 88.4838, Test Loss: 1.0194, Test Accuracy: 89.9023\n",
      "Epoch [298/1000], Train Loss: 1.0416, Train Accuracy: 88.4838, Test Loss: 1.0192, Test Accuracy: 89.9023\n",
      "Epoch [299/1000], Train Loss: 1.0414, Train Accuracy: 88.4838, Test Loss: 1.0189, Test Accuracy: 89.9023\n",
      "Epoch [300/1000], Train Loss: 1.0412, Train Accuracy: 88.4838, Test Loss: 1.0187, Test Accuracy: 89.9023\n",
      "Epoch [301/1000], Train Loss: 1.0409, Train Accuracy: 88.4838, Test Loss: 1.0185, Test Accuracy: 89.9023\n",
      "Epoch [302/1000], Train Loss: 1.0407, Train Accuracy: 88.4838, Test Loss: 1.0183, Test Accuracy: 89.9023\n",
      "Epoch [303/1000], Train Loss: 1.0405, Train Accuracy: 88.4838, Test Loss: 1.0181, Test Accuracy: 89.9023\n",
      "Epoch [304/1000], Train Loss: 1.0403, Train Accuracy: 88.4838, Test Loss: 1.0178, Test Accuracy: 89.9023\n",
      "Epoch [305/1000], Train Loss: 1.0401, Train Accuracy: 88.4838, Test Loss: 1.0176, Test Accuracy: 89.9023\n",
      "Epoch [306/1000], Train Loss: 1.0398, Train Accuracy: 88.4838, Test Loss: 1.0174, Test Accuracy: 89.9023\n",
      "Epoch [307/1000], Train Loss: 1.0396, Train Accuracy: 88.4838, Test Loss: 1.0172, Test Accuracy: 89.9023\n",
      "Epoch [308/1000], Train Loss: 1.0394, Train Accuracy: 88.4838, Test Loss: 1.0169, Test Accuracy: 89.9023\n",
      "Epoch [309/1000], Train Loss: 1.0392, Train Accuracy: 88.4838, Test Loss: 1.0167, Test Accuracy: 89.9023\n",
      "Epoch [310/1000], Train Loss: 1.0390, Train Accuracy: 88.4838, Test Loss: 1.0165, Test Accuracy: 89.9023\n",
      "Epoch [311/1000], Train Loss: 1.0387, Train Accuracy: 88.4838, Test Loss: 1.0163, Test Accuracy: 89.9023\n",
      "Epoch [312/1000], Train Loss: 1.0385, Train Accuracy: 88.4838, Test Loss: 1.0160, Test Accuracy: 89.9023\n",
      "Epoch [313/1000], Train Loss: 1.0383, Train Accuracy: 88.4838, Test Loss: 1.0158, Test Accuracy: 89.9023\n",
      "Epoch [314/1000], Train Loss: 1.0381, Train Accuracy: 88.4838, Test Loss: 1.0156, Test Accuracy: 89.9023\n",
      "Epoch [315/1000], Train Loss: 1.0379, Train Accuracy: 88.4838, Test Loss: 1.0154, Test Accuracy: 89.9023\n",
      "Epoch [316/1000], Train Loss: 1.0376, Train Accuracy: 88.4838, Test Loss: 1.0152, Test Accuracy: 89.9023\n",
      "Epoch [317/1000], Train Loss: 1.0374, Train Accuracy: 88.4838, Test Loss: 1.0149, Test Accuracy: 89.9023\n",
      "Epoch [318/1000], Train Loss: 1.0372, Train Accuracy: 88.4838, Test Loss: 1.0147, Test Accuracy: 89.9023\n",
      "Epoch [319/1000], Train Loss: 1.0370, Train Accuracy: 88.4838, Test Loss: 1.0145, Test Accuracy: 89.9023\n",
      "Epoch [320/1000], Train Loss: 1.0368, Train Accuracy: 88.4838, Test Loss: 1.0143, Test Accuracy: 89.9023\n",
      "Epoch [321/1000], Train Loss: 1.0366, Train Accuracy: 88.4838, Test Loss: 1.0140, Test Accuracy: 89.9023\n",
      "Epoch [322/1000], Train Loss: 1.0363, Train Accuracy: 88.4838, Test Loss: 1.0138, Test Accuracy: 89.9023\n",
      "Epoch [323/1000], Train Loss: 1.0361, Train Accuracy: 88.4838, Test Loss: 1.0136, Test Accuracy: 89.9023\n",
      "Epoch [324/1000], Train Loss: 1.0359, Train Accuracy: 88.4838, Test Loss: 1.0134, Test Accuracy: 89.9023\n",
      "Epoch [325/1000], Train Loss: 1.0357, Train Accuracy: 88.4838, Test Loss: 1.0132, Test Accuracy: 89.9023\n",
      "Epoch [326/1000], Train Loss: 1.0355, Train Accuracy: 88.4838, Test Loss: 1.0130, Test Accuracy: 89.9023\n",
      "Epoch [327/1000], Train Loss: 1.0352, Train Accuracy: 88.4838, Test Loss: 1.0127, Test Accuracy: 89.9023\n",
      "Epoch [328/1000], Train Loss: 1.0350, Train Accuracy: 88.4838, Test Loss: 1.0125, Test Accuracy: 89.9023\n",
      "Epoch [329/1000], Train Loss: 1.0348, Train Accuracy: 88.4838, Test Loss: 1.0123, Test Accuracy: 89.9023\n",
      "Epoch [330/1000], Train Loss: 1.0346, Train Accuracy: 88.4838, Test Loss: 1.0121, Test Accuracy: 89.9023\n",
      "Epoch [331/1000], Train Loss: 1.0344, Train Accuracy: 88.4838, Test Loss: 1.0119, Test Accuracy: 89.9023\n",
      "Epoch [332/1000], Train Loss: 1.0341, Train Accuracy: 88.4838, Test Loss: 1.0116, Test Accuracy: 89.9023\n",
      "Epoch [333/1000], Train Loss: 1.0339, Train Accuracy: 88.4838, Test Loss: 1.0114, Test Accuracy: 89.9023\n",
      "Epoch [334/1000], Train Loss: 1.0337, Train Accuracy: 88.4838, Test Loss: 1.0112, Test Accuracy: 89.9023\n",
      "Epoch [335/1000], Train Loss: 1.0335, Train Accuracy: 88.4838, Test Loss: 1.0110, Test Accuracy: 89.9023\n",
      "Epoch [336/1000], Train Loss: 1.0333, Train Accuracy: 88.4838, Test Loss: 1.0108, Test Accuracy: 89.9023\n",
      "Epoch [337/1000], Train Loss: 1.0330, Train Accuracy: 88.4838, Test Loss: 1.0106, Test Accuracy: 89.9023\n",
      "Epoch [338/1000], Train Loss: 1.0328, Train Accuracy: 88.4838, Test Loss: 1.0103, Test Accuracy: 89.9023\n",
      "Epoch [339/1000], Train Loss: 1.0326, Train Accuracy: 88.4838, Test Loss: 1.0101, Test Accuracy: 89.9023\n",
      "Epoch [340/1000], Train Loss: 1.0324, Train Accuracy: 88.4838, Test Loss: 1.0099, Test Accuracy: 89.9023\n",
      "Epoch [341/1000], Train Loss: 1.0322, Train Accuracy: 88.4838, Test Loss: 1.0097, Test Accuracy: 89.9023\n",
      "Epoch [342/1000], Train Loss: 1.0319, Train Accuracy: 88.4838, Test Loss: 1.0095, Test Accuracy: 89.9023\n",
      "Epoch [343/1000], Train Loss: 1.0317, Train Accuracy: 88.4838, Test Loss: 1.0093, Test Accuracy: 89.9023\n",
      "Epoch [344/1000], Train Loss: 1.0315, Train Accuracy: 88.4838, Test Loss: 1.0091, Test Accuracy: 89.9023\n",
      "Epoch [345/1000], Train Loss: 1.0313, Train Accuracy: 88.4838, Test Loss: 1.0088, Test Accuracy: 89.9023\n",
      "Epoch [346/1000], Train Loss: 1.0311, Train Accuracy: 88.4838, Test Loss: 1.0086, Test Accuracy: 89.9023\n",
      "Epoch [347/1000], Train Loss: 1.0309, Train Accuracy: 88.4838, Test Loss: 1.0084, Test Accuracy: 89.9023\n",
      "Epoch [348/1000], Train Loss: 1.0306, Train Accuracy: 88.4838, Test Loss: 1.0082, Test Accuracy: 89.9023\n",
      "Epoch [349/1000], Train Loss: 1.0304, Train Accuracy: 88.4838, Test Loss: 1.0080, Test Accuracy: 89.9023\n",
      "Epoch [350/1000], Train Loss: 1.0302, Train Accuracy: 88.4838, Test Loss: 1.0078, Test Accuracy: 89.9023\n",
      "Epoch [351/1000], Train Loss: 1.0300, Train Accuracy: 88.4838, Test Loss: 1.0076, Test Accuracy: 89.9023\n",
      "Epoch [352/1000], Train Loss: 1.0298, Train Accuracy: 88.4838, Test Loss: 1.0073, Test Accuracy: 89.9023\n",
      "Epoch [353/1000], Train Loss: 1.0296, Train Accuracy: 88.4838, Test Loss: 1.0071, Test Accuracy: 89.9023\n",
      "Epoch [354/1000], Train Loss: 1.0293, Train Accuracy: 88.4838, Test Loss: 1.0069, Test Accuracy: 89.9023\n",
      "Epoch [355/1000], Train Loss: 1.0291, Train Accuracy: 88.4838, Test Loss: 1.0067, Test Accuracy: 89.9023\n",
      "Epoch [356/1000], Train Loss: 1.0289, Train Accuracy: 88.4838, Test Loss: 1.0065, Test Accuracy: 89.9023\n",
      "Epoch [357/1000], Train Loss: 1.0287, Train Accuracy: 88.4838, Test Loss: 1.0063, Test Accuracy: 89.9023\n",
      "Epoch [358/1000], Train Loss: 1.0285, Train Accuracy: 88.4838, Test Loss: 1.0061, Test Accuracy: 89.9023\n",
      "Epoch [359/1000], Train Loss: 1.0283, Train Accuracy: 88.4838, Test Loss: 1.0059, Test Accuracy: 89.9023\n",
      "Epoch [360/1000], Train Loss: 1.0280, Train Accuracy: 88.4838, Test Loss: 1.0057, Test Accuracy: 89.9023\n",
      "Epoch [361/1000], Train Loss: 1.0278, Train Accuracy: 88.4838, Test Loss: 1.0055, Test Accuracy: 89.9023\n",
      "Epoch [362/1000], Train Loss: 1.0276, Train Accuracy: 88.4838, Test Loss: 1.0052, Test Accuracy: 89.9023\n",
      "Epoch [363/1000], Train Loss: 1.0274, Train Accuracy: 88.4838, Test Loss: 1.0050, Test Accuracy: 89.9023\n",
      "Epoch [364/1000], Train Loss: 1.0272, Train Accuracy: 88.4838, Test Loss: 1.0048, Test Accuracy: 89.9023\n",
      "Epoch [365/1000], Train Loss: 1.0270, Train Accuracy: 88.4838, Test Loss: 1.0046, Test Accuracy: 89.9023\n",
      "Epoch [366/1000], Train Loss: 1.0267, Train Accuracy: 88.4838, Test Loss: 1.0044, Test Accuracy: 89.9023\n",
      "Epoch [367/1000], Train Loss: 1.0265, Train Accuracy: 88.4838, Test Loss: 1.0042, Test Accuracy: 89.9023\n",
      "Epoch [368/1000], Train Loss: 1.0263, Train Accuracy: 88.4838, Test Loss: 1.0040, Test Accuracy: 89.9023\n",
      "Epoch [369/1000], Train Loss: 1.0261, Train Accuracy: 88.4838, Test Loss: 1.0038, Test Accuracy: 89.9023\n",
      "Epoch [370/1000], Train Loss: 1.0259, Train Accuracy: 88.4838, Test Loss: 1.0036, Test Accuracy: 89.9023\n",
      "Epoch [371/1000], Train Loss: 1.0257, Train Accuracy: 88.4838, Test Loss: 1.0034, Test Accuracy: 89.9023\n",
      "Epoch [372/1000], Train Loss: 1.0255, Train Accuracy: 88.4838, Test Loss: 1.0032, Test Accuracy: 89.9023\n",
      "Epoch [373/1000], Train Loss: 1.0253, Train Accuracy: 88.4838, Test Loss: 1.0030, Test Accuracy: 89.9023\n",
      "Epoch [374/1000], Train Loss: 1.0250, Train Accuracy: 88.4838, Test Loss: 1.0028, Test Accuracy: 89.9023\n",
      "Epoch [375/1000], Train Loss: 1.0248, Train Accuracy: 88.4838, Test Loss: 1.0026, Test Accuracy: 89.9023\n",
      "Epoch [376/1000], Train Loss: 1.0246, Train Accuracy: 88.4838, Test Loss: 1.0024, Test Accuracy: 89.9023\n",
      "Epoch [377/1000], Train Loss: 1.0244, Train Accuracy: 88.4838, Test Loss: 1.0022, Test Accuracy: 89.9023\n",
      "Epoch [378/1000], Train Loss: 1.0242, Train Accuracy: 88.4838, Test Loss: 1.0019, Test Accuracy: 89.9023\n",
      "Epoch [379/1000], Train Loss: 1.0240, Train Accuracy: 88.4838, Test Loss: 1.0017, Test Accuracy: 89.9023\n",
      "Epoch [380/1000], Train Loss: 1.0238, Train Accuracy: 88.4838, Test Loss: 1.0015, Test Accuracy: 89.9023\n",
      "Epoch [381/1000], Train Loss: 1.0236, Train Accuracy: 88.4838, Test Loss: 1.0013, Test Accuracy: 89.9023\n",
      "Epoch [382/1000], Train Loss: 1.0234, Train Accuracy: 88.4838, Test Loss: 1.0011, Test Accuracy: 89.9023\n",
      "Epoch [383/1000], Train Loss: 1.0231, Train Accuracy: 88.4838, Test Loss: 1.0009, Test Accuracy: 89.9023\n",
      "Epoch [384/1000], Train Loss: 1.0229, Train Accuracy: 88.4838, Test Loss: 1.0007, Test Accuracy: 89.9023\n",
      "Epoch [385/1000], Train Loss: 1.0227, Train Accuracy: 88.4838, Test Loss: 1.0005, Test Accuracy: 89.9023\n",
      "Epoch [386/1000], Train Loss: 1.0225, Train Accuracy: 88.4838, Test Loss: 1.0003, Test Accuracy: 89.9023\n",
      "Epoch [387/1000], Train Loss: 1.0223, Train Accuracy: 88.4838, Test Loss: 1.0001, Test Accuracy: 89.9023\n",
      "Epoch [388/1000], Train Loss: 1.0221, Train Accuracy: 88.4838, Test Loss: 0.9999, Test Accuracy: 89.9023\n",
      "Epoch [389/1000], Train Loss: 1.0219, Train Accuracy: 88.4838, Test Loss: 0.9997, Test Accuracy: 89.9023\n",
      "Epoch [390/1000], Train Loss: 1.0217, Train Accuracy: 88.4838, Test Loss: 0.9995, Test Accuracy: 89.9023\n",
      "Epoch [391/1000], Train Loss: 1.0215, Train Accuracy: 88.4838, Test Loss: 0.9994, Test Accuracy: 89.9023\n",
      "Epoch [392/1000], Train Loss: 1.0213, Train Accuracy: 88.4838, Test Loss: 0.9992, Test Accuracy: 89.9023\n",
      "Epoch [393/1000], Train Loss: 1.0211, Train Accuracy: 88.4838, Test Loss: 0.9990, Test Accuracy: 89.9023\n",
      "Epoch [394/1000], Train Loss: 1.0209, Train Accuracy: 88.4838, Test Loss: 0.9988, Test Accuracy: 89.9023\n",
      "Epoch [395/1000], Train Loss: 1.0207, Train Accuracy: 88.4838, Test Loss: 0.9986, Test Accuracy: 89.9023\n",
      "Epoch [396/1000], Train Loss: 1.0204, Train Accuracy: 88.4838, Test Loss: 0.9984, Test Accuracy: 89.9023\n",
      "Epoch [397/1000], Train Loss: 1.0202, Train Accuracy: 88.4838, Test Loss: 0.9982, Test Accuracy: 89.9023\n",
      "Epoch [398/1000], Train Loss: 1.0200, Train Accuracy: 88.4838, Test Loss: 0.9980, Test Accuracy: 89.9023\n",
      "Epoch [399/1000], Train Loss: 1.0198, Train Accuracy: 88.4838, Test Loss: 0.9978, Test Accuracy: 89.9023\n",
      "Epoch [400/1000], Train Loss: 1.0196, Train Accuracy: 88.4838, Test Loss: 0.9976, Test Accuracy: 89.9023\n",
      "Epoch [401/1000], Train Loss: 1.0194, Train Accuracy: 88.4838, Test Loss: 0.9974, Test Accuracy: 89.9023\n",
      "Epoch [402/1000], Train Loss: 1.0192, Train Accuracy: 88.4838, Test Loss: 0.9972, Test Accuracy: 89.9023\n",
      "Epoch [403/1000], Train Loss: 1.0190, Train Accuracy: 88.4838, Test Loss: 0.9970, Test Accuracy: 89.9023\n",
      "Epoch [404/1000], Train Loss: 1.0188, Train Accuracy: 88.4838, Test Loss: 0.9968, Test Accuracy: 89.9023\n",
      "Epoch [405/1000], Train Loss: 1.0186, Train Accuracy: 88.4838, Test Loss: 0.9966, Test Accuracy: 89.9023\n",
      "Epoch [406/1000], Train Loss: 1.0184, Train Accuracy: 88.4838, Test Loss: 0.9965, Test Accuracy: 89.9023\n",
      "Epoch [407/1000], Train Loss: 1.0182, Train Accuracy: 88.4838, Test Loss: 0.9963, Test Accuracy: 89.9023\n",
      "Epoch [408/1000], Train Loss: 1.0180, Train Accuracy: 88.4838, Test Loss: 0.9961, Test Accuracy: 89.9023\n",
      "Epoch [409/1000], Train Loss: 1.0178, Train Accuracy: 88.4838, Test Loss: 0.9959, Test Accuracy: 89.9023\n",
      "Epoch [410/1000], Train Loss: 1.0176, Train Accuracy: 88.4838, Test Loss: 0.9957, Test Accuracy: 89.9023\n",
      "Epoch [411/1000], Train Loss: 1.0174, Train Accuracy: 88.4838, Test Loss: 0.9955, Test Accuracy: 89.9023\n",
      "Epoch [412/1000], Train Loss: 1.0172, Train Accuracy: 88.4838, Test Loss: 0.9953, Test Accuracy: 89.9023\n",
      "Epoch [413/1000], Train Loss: 1.0170, Train Accuracy: 88.4838, Test Loss: 0.9951, Test Accuracy: 89.9023\n",
      "Epoch [414/1000], Train Loss: 1.0168, Train Accuracy: 88.4838, Test Loss: 0.9950, Test Accuracy: 89.9023\n",
      "Epoch [415/1000], Train Loss: 1.0166, Train Accuracy: 88.4838, Test Loss: 0.9948, Test Accuracy: 89.9023\n",
      "Epoch [416/1000], Train Loss: 1.0164, Train Accuracy: 88.4838, Test Loss: 0.9946, Test Accuracy: 89.9023\n",
      "Epoch [417/1000], Train Loss: 1.0162, Train Accuracy: 88.4838, Test Loss: 0.9944, Test Accuracy: 89.9023\n",
      "Epoch [418/1000], Train Loss: 1.0160, Train Accuracy: 88.4838, Test Loss: 0.9942, Test Accuracy: 89.9023\n",
      "Epoch [419/1000], Train Loss: 1.0158, Train Accuracy: 88.4838, Test Loss: 0.9940, Test Accuracy: 89.9023\n",
      "Epoch [420/1000], Train Loss: 1.0156, Train Accuracy: 88.4838, Test Loss: 0.9939, Test Accuracy: 89.9023\n",
      "Epoch [421/1000], Train Loss: 1.0154, Train Accuracy: 88.4838, Test Loss: 0.9937, Test Accuracy: 89.9023\n",
      "Epoch [422/1000], Train Loss: 1.0152, Train Accuracy: 88.4838, Test Loss: 0.9935, Test Accuracy: 89.9023\n",
      "Epoch [423/1000], Train Loss: 1.0150, Train Accuracy: 88.4838, Test Loss: 0.9933, Test Accuracy: 89.9023\n",
      "Epoch [424/1000], Train Loss: 1.0148, Train Accuracy: 88.4838, Test Loss: 0.9931, Test Accuracy: 89.9023\n",
      "Epoch [425/1000], Train Loss: 1.0146, Train Accuracy: 88.4838, Test Loss: 0.9930, Test Accuracy: 89.9023\n",
      "Epoch [426/1000], Train Loss: 1.0144, Train Accuracy: 88.4838, Test Loss: 0.9928, Test Accuracy: 89.9023\n",
      "Epoch [427/1000], Train Loss: 1.0142, Train Accuracy: 88.4838, Test Loss: 0.9926, Test Accuracy: 89.9023\n",
      "Epoch [428/1000], Train Loss: 1.0141, Train Accuracy: 88.4838, Test Loss: 0.9924, Test Accuracy: 89.9023\n",
      "Epoch [429/1000], Train Loss: 1.0139, Train Accuracy: 88.4838, Test Loss: 0.9922, Test Accuracy: 89.9023\n",
      "Epoch [430/1000], Train Loss: 1.0137, Train Accuracy: 88.4838, Test Loss: 0.9921, Test Accuracy: 89.9023\n",
      "Epoch [431/1000], Train Loss: 1.0135, Train Accuracy: 88.4838, Test Loss: 0.9919, Test Accuracy: 89.9023\n",
      "Epoch [432/1000], Train Loss: 1.0133, Train Accuracy: 88.4838, Test Loss: 0.9917, Test Accuracy: 89.9023\n",
      "Epoch [433/1000], Train Loss: 1.0131, Train Accuracy: 88.4838, Test Loss: 0.9915, Test Accuracy: 89.9023\n",
      "Epoch [434/1000], Train Loss: 1.0129, Train Accuracy: 88.4838, Test Loss: 0.9914, Test Accuracy: 89.9023\n",
      "Epoch [435/1000], Train Loss: 1.0127, Train Accuracy: 88.4838, Test Loss: 0.9912, Test Accuracy: 89.9023\n",
      "Epoch [436/1000], Train Loss: 1.0125, Train Accuracy: 88.4838, Test Loss: 0.9910, Test Accuracy: 89.9023\n",
      "Epoch [437/1000], Train Loss: 1.0123, Train Accuracy: 88.4838, Test Loss: 0.9908, Test Accuracy: 89.9023\n",
      "Epoch [438/1000], Train Loss: 1.0121, Train Accuracy: 88.4838, Test Loss: 0.9907, Test Accuracy: 89.9023\n",
      "Epoch [439/1000], Train Loss: 1.0119, Train Accuracy: 88.4838, Test Loss: 0.9905, Test Accuracy: 89.9023\n",
      "Epoch [440/1000], Train Loss: 1.0118, Train Accuracy: 88.4838, Test Loss: 0.9903, Test Accuracy: 89.9023\n",
      "Epoch [441/1000], Train Loss: 1.0116, Train Accuracy: 88.4838, Test Loss: 0.9902, Test Accuracy: 89.9023\n",
      "Epoch [442/1000], Train Loss: 1.0114, Train Accuracy: 88.4838, Test Loss: 0.9900, Test Accuracy: 89.9023\n",
      "Epoch [443/1000], Train Loss: 1.0112, Train Accuracy: 88.4838, Test Loss: 0.9898, Test Accuracy: 89.9023\n",
      "Epoch [444/1000], Train Loss: 1.0110, Train Accuracy: 88.4838, Test Loss: 0.9896, Test Accuracy: 89.9023\n",
      "Epoch [445/1000], Train Loss: 1.0108, Train Accuracy: 88.4838, Test Loss: 0.9895, Test Accuracy: 89.9023\n",
      "Epoch [446/1000], Train Loss: 1.0106, Train Accuracy: 88.4838, Test Loss: 0.9893, Test Accuracy: 89.9023\n",
      "Epoch [447/1000], Train Loss: 1.0105, Train Accuracy: 88.4838, Test Loss: 0.9891, Test Accuracy: 89.9023\n",
      "Epoch [448/1000], Train Loss: 1.0103, Train Accuracy: 88.4838, Test Loss: 0.9890, Test Accuracy: 89.9023\n",
      "Epoch [449/1000], Train Loss: 1.0101, Train Accuracy: 88.4838, Test Loss: 0.9888, Test Accuracy: 89.9023\n",
      "Epoch [450/1000], Train Loss: 1.0099, Train Accuracy: 88.4838, Test Loss: 0.9886, Test Accuracy: 89.9023\n",
      "Epoch [451/1000], Train Loss: 1.0097, Train Accuracy: 88.4838, Test Loss: 0.9885, Test Accuracy: 89.9023\n",
      "Epoch [452/1000], Train Loss: 1.0095, Train Accuracy: 88.4838, Test Loss: 0.9883, Test Accuracy: 89.9023\n",
      "Epoch [453/1000], Train Loss: 1.0094, Train Accuracy: 88.4838, Test Loss: 0.9881, Test Accuracy: 89.9023\n",
      "Epoch [454/1000], Train Loss: 1.0092, Train Accuracy: 88.4838, Test Loss: 0.9880, Test Accuracy: 89.9023\n",
      "Epoch [455/1000], Train Loss: 1.0090, Train Accuracy: 88.4838, Test Loss: 0.9878, Test Accuracy: 89.9023\n",
      "Epoch [456/1000], Train Loss: 1.0088, Train Accuracy: 88.4838, Test Loss: 0.9877, Test Accuracy: 89.9023\n",
      "Epoch [457/1000], Train Loss: 1.0086, Train Accuracy: 88.4838, Test Loss: 0.9875, Test Accuracy: 89.9023\n",
      "Epoch [458/1000], Train Loss: 1.0084, Train Accuracy: 88.4838, Test Loss: 0.9873, Test Accuracy: 89.9023\n",
      "Epoch [459/1000], Train Loss: 1.0083, Train Accuracy: 88.4838, Test Loss: 0.9872, Test Accuracy: 89.9023\n",
      "Epoch [460/1000], Train Loss: 1.0081, Train Accuracy: 88.4838, Test Loss: 0.9870, Test Accuracy: 89.9023\n",
      "Epoch [461/1000], Train Loss: 1.0079, Train Accuracy: 88.4838, Test Loss: 0.9868, Test Accuracy: 89.9023\n",
      "Epoch [462/1000], Train Loss: 1.0077, Train Accuracy: 88.4838, Test Loss: 0.9867, Test Accuracy: 89.9023\n",
      "Epoch [463/1000], Train Loss: 1.0076, Train Accuracy: 88.4838, Test Loss: 0.9865, Test Accuracy: 89.9023\n",
      "Epoch [464/1000], Train Loss: 1.0074, Train Accuracy: 88.4838, Test Loss: 0.9864, Test Accuracy: 89.9023\n",
      "Epoch [465/1000], Train Loss: 1.0072, Train Accuracy: 88.4838, Test Loss: 0.9862, Test Accuracy: 89.9023\n",
      "Epoch [466/1000], Train Loss: 1.0070, Train Accuracy: 88.4838, Test Loss: 0.9861, Test Accuracy: 89.9023\n",
      "Epoch [467/1000], Train Loss: 1.0069, Train Accuracy: 88.4838, Test Loss: 0.9859, Test Accuracy: 89.9023\n",
      "Epoch [468/1000], Train Loss: 1.0067, Train Accuracy: 88.4838, Test Loss: 0.9857, Test Accuracy: 89.9023\n",
      "Epoch [469/1000], Train Loss: 1.0065, Train Accuracy: 88.4838, Test Loss: 0.9856, Test Accuracy: 89.9023\n",
      "Epoch [470/1000], Train Loss: 1.0063, Train Accuracy: 88.4838, Test Loss: 0.9854, Test Accuracy: 89.9023\n",
      "Epoch [471/1000], Train Loss: 1.0062, Train Accuracy: 88.4838, Test Loss: 0.9853, Test Accuracy: 89.9023\n",
      "Epoch [472/1000], Train Loss: 1.0060, Train Accuracy: 88.4838, Test Loss: 0.9851, Test Accuracy: 89.9023\n",
      "Epoch [473/1000], Train Loss: 1.0058, Train Accuracy: 88.4838, Test Loss: 0.9850, Test Accuracy: 89.9023\n",
      "Epoch [474/1000], Train Loss: 1.0057, Train Accuracy: 88.4838, Test Loss: 0.9848, Test Accuracy: 89.9023\n",
      "Epoch [475/1000], Train Loss: 1.0055, Train Accuracy: 88.4838, Test Loss: 0.9847, Test Accuracy: 89.9023\n",
      "Epoch [476/1000], Train Loss: 1.0053, Train Accuracy: 88.4838, Test Loss: 0.9845, Test Accuracy: 89.9023\n",
      "Epoch [477/1000], Train Loss: 1.0052, Train Accuracy: 88.4838, Test Loss: 0.9844, Test Accuracy: 89.9023\n",
      "Epoch [478/1000], Train Loss: 1.0050, Train Accuracy: 88.4838, Test Loss: 0.9842, Test Accuracy: 89.9023\n",
      "Epoch [479/1000], Train Loss: 1.0048, Train Accuracy: 88.4838, Test Loss: 0.9841, Test Accuracy: 89.9023\n",
      "Epoch [480/1000], Train Loss: 1.0047, Train Accuracy: 88.4838, Test Loss: 0.9839, Test Accuracy: 89.9023\n",
      "Epoch [481/1000], Train Loss: 1.0045, Train Accuracy: 88.4838, Test Loss: 0.9838, Test Accuracy: 89.9023\n",
      "Epoch [482/1000], Train Loss: 1.0043, Train Accuracy: 88.4838, Test Loss: 0.9836, Test Accuracy: 89.9023\n",
      "Epoch [483/1000], Train Loss: 1.0042, Train Accuracy: 88.4838, Test Loss: 0.9835, Test Accuracy: 89.9023\n",
      "Epoch [484/1000], Train Loss: 1.0040, Train Accuracy: 88.4838, Test Loss: 0.9833, Test Accuracy: 89.9023\n",
      "Epoch [485/1000], Train Loss: 1.0038, Train Accuracy: 88.4838, Test Loss: 0.9832, Test Accuracy: 89.9023\n",
      "Epoch [486/1000], Train Loss: 1.0037, Train Accuracy: 88.4838, Test Loss: 0.9831, Test Accuracy: 89.9023\n",
      "Epoch [487/1000], Train Loss: 1.0035, Train Accuracy: 88.4838, Test Loss: 0.9829, Test Accuracy: 89.9023\n",
      "Epoch [488/1000], Train Loss: 1.0033, Train Accuracy: 88.4838, Test Loss: 0.9828, Test Accuracy: 89.9023\n",
      "Epoch [489/1000], Train Loss: 1.0032, Train Accuracy: 88.4838, Test Loss: 0.9826, Test Accuracy: 89.9023\n",
      "Epoch [490/1000], Train Loss: 1.0030, Train Accuracy: 88.4838, Test Loss: 0.9825, Test Accuracy: 89.9023\n",
      "Epoch [491/1000], Train Loss: 1.0029, Train Accuracy: 88.4838, Test Loss: 0.9824, Test Accuracy: 89.9023\n",
      "Epoch [492/1000], Train Loss: 1.0027, Train Accuracy: 88.4838, Test Loss: 0.9822, Test Accuracy: 89.9023\n",
      "Epoch [493/1000], Train Loss: 1.0025, Train Accuracy: 88.4838, Test Loss: 0.9821, Test Accuracy: 89.9023\n",
      "Epoch [494/1000], Train Loss: 1.0024, Train Accuracy: 88.4838, Test Loss: 0.9819, Test Accuracy: 89.9023\n",
      "Epoch [495/1000], Train Loss: 1.0022, Train Accuracy: 88.4838, Test Loss: 0.9818, Test Accuracy: 89.9023\n",
      "Epoch [496/1000], Train Loss: 1.0021, Train Accuracy: 88.4838, Test Loss: 0.9817, Test Accuracy: 89.9023\n",
      "Epoch [497/1000], Train Loss: 1.0019, Train Accuracy: 88.4838, Test Loss: 0.9815, Test Accuracy: 89.9023\n",
      "Epoch [498/1000], Train Loss: 1.0018, Train Accuracy: 88.4838, Test Loss: 0.9814, Test Accuracy: 89.9023\n",
      "Epoch [499/1000], Train Loss: 1.0016, Train Accuracy: 88.4838, Test Loss: 0.9812, Test Accuracy: 89.9023\n",
      "Epoch [500/1000], Train Loss: 1.0014, Train Accuracy: 88.4838, Test Loss: 0.9811, Test Accuracy: 89.9023\n",
      "Epoch [501/1000], Train Loss: 1.0013, Train Accuracy: 88.4838, Test Loss: 0.9810, Test Accuracy: 89.9023\n",
      "Epoch [502/1000], Train Loss: 1.0011, Train Accuracy: 88.4838, Test Loss: 0.9808, Test Accuracy: 89.9023\n",
      "Epoch [503/1000], Train Loss: 1.0010, Train Accuracy: 88.4838, Test Loss: 0.9807, Test Accuracy: 89.9023\n",
      "Epoch [504/1000], Train Loss: 1.0008, Train Accuracy: 88.4838, Test Loss: 0.9806, Test Accuracy: 89.9023\n",
      "Epoch [505/1000], Train Loss: 1.0007, Train Accuracy: 88.4838, Test Loss: 0.9804, Test Accuracy: 89.9023\n",
      "Epoch [506/1000], Train Loss: 1.0005, Train Accuracy: 88.4838, Test Loss: 0.9803, Test Accuracy: 89.9023\n",
      "Epoch [507/1000], Train Loss: 1.0004, Train Accuracy: 88.4838, Test Loss: 0.9802, Test Accuracy: 89.9023\n",
      "Epoch [508/1000], Train Loss: 1.0002, Train Accuracy: 88.4838, Test Loss: 0.9800, Test Accuracy: 89.9023\n",
      "Epoch [509/1000], Train Loss: 1.0001, Train Accuracy: 88.4838, Test Loss: 0.9799, Test Accuracy: 89.9023\n",
      "Epoch [510/1000], Train Loss: 0.9999, Train Accuracy: 88.4838, Test Loss: 0.9798, Test Accuracy: 89.9023\n",
      "Epoch [511/1000], Train Loss: 0.9998, Train Accuracy: 88.4838, Test Loss: 0.9797, Test Accuracy: 89.9023\n",
      "Epoch [512/1000], Train Loss: 0.9996, Train Accuracy: 88.4838, Test Loss: 0.9795, Test Accuracy: 89.9023\n",
      "Epoch [513/1000], Train Loss: 0.9995, Train Accuracy: 88.4838, Test Loss: 0.9794, Test Accuracy: 89.9023\n",
      "Epoch [514/1000], Train Loss: 0.9993, Train Accuracy: 88.4838, Test Loss: 0.9793, Test Accuracy: 89.9023\n",
      "Epoch [515/1000], Train Loss: 0.9992, Train Accuracy: 88.4838, Test Loss: 0.9791, Test Accuracy: 89.9023\n",
      "Epoch [516/1000], Train Loss: 0.9990, Train Accuracy: 88.4838, Test Loss: 0.9790, Test Accuracy: 89.9023\n",
      "Epoch [517/1000], Train Loss: 0.9989, Train Accuracy: 88.4838, Test Loss: 0.9789, Test Accuracy: 89.9023\n",
      "Epoch [518/1000], Train Loss: 0.9987, Train Accuracy: 88.4838, Test Loss: 0.9788, Test Accuracy: 89.9023\n",
      "Epoch [519/1000], Train Loss: 0.9986, Train Accuracy: 88.4838, Test Loss: 0.9786, Test Accuracy: 89.9023\n",
      "Epoch [520/1000], Train Loss: 0.9985, Train Accuracy: 88.4838, Test Loss: 0.9785, Test Accuracy: 89.9023\n",
      "Epoch [521/1000], Train Loss: 0.9983, Train Accuracy: 88.4838, Test Loss: 0.9784, Test Accuracy: 89.9023\n",
      "Epoch [522/1000], Train Loss: 0.9982, Train Accuracy: 88.4838, Test Loss: 0.9783, Test Accuracy: 89.9023\n",
      "Epoch [523/1000], Train Loss: 0.9980, Train Accuracy: 88.4838, Test Loss: 0.9781, Test Accuracy: 89.9023\n",
      "Epoch [524/1000], Train Loss: 0.9979, Train Accuracy: 88.4838, Test Loss: 0.9780, Test Accuracy: 89.9023\n",
      "Epoch [525/1000], Train Loss: 0.9977, Train Accuracy: 88.4838, Test Loss: 0.9779, Test Accuracy: 89.9023\n",
      "Epoch [526/1000], Train Loss: 0.9976, Train Accuracy: 88.4838, Test Loss: 0.9778, Test Accuracy: 89.9023\n",
      "Epoch [527/1000], Train Loss: 0.9975, Train Accuracy: 88.4838, Test Loss: 0.9777, Test Accuracy: 89.9023\n",
      "Epoch [528/1000], Train Loss: 0.9973, Train Accuracy: 88.4838, Test Loss: 0.9775, Test Accuracy: 89.9023\n",
      "Epoch [529/1000], Train Loss: 0.9972, Train Accuracy: 88.4838, Test Loss: 0.9774, Test Accuracy: 89.9023\n",
      "Epoch [530/1000], Train Loss: 0.9971, Train Accuracy: 88.4838, Test Loss: 0.9773, Test Accuracy: 89.9023\n",
      "Epoch [531/1000], Train Loss: 0.9969, Train Accuracy: 88.4838, Test Loss: 0.9772, Test Accuracy: 89.9023\n",
      "Epoch [532/1000], Train Loss: 0.9968, Train Accuracy: 88.4838, Test Loss: 0.9771, Test Accuracy: 89.9023\n",
      "Epoch [533/1000], Train Loss: 0.9966, Train Accuracy: 88.4838, Test Loss: 0.9769, Test Accuracy: 89.9023\n",
      "Epoch [534/1000], Train Loss: 0.9965, Train Accuracy: 88.4838, Test Loss: 0.9768, Test Accuracy: 89.9023\n",
      "Epoch [535/1000], Train Loss: 0.9964, Train Accuracy: 88.4838, Test Loss: 0.9767, Test Accuracy: 89.9023\n",
      "Epoch [536/1000], Train Loss: 0.9962, Train Accuracy: 88.4838, Test Loss: 0.9766, Test Accuracy: 89.9023\n",
      "Epoch [537/1000], Train Loss: 0.9961, Train Accuracy: 88.4838, Test Loss: 0.9765, Test Accuracy: 89.9023\n",
      "Epoch [538/1000], Train Loss: 0.9960, Train Accuracy: 88.4838, Test Loss: 0.9764, Test Accuracy: 89.9023\n",
      "Epoch [539/1000], Train Loss: 0.9958, Train Accuracy: 88.4838, Test Loss: 0.9763, Test Accuracy: 89.9023\n",
      "Epoch [540/1000], Train Loss: 0.9957, Train Accuracy: 88.4838, Test Loss: 0.9761, Test Accuracy: 89.9023\n",
      "Epoch [541/1000], Train Loss: 0.9956, Train Accuracy: 88.4838, Test Loss: 0.9760, Test Accuracy: 89.9023\n",
      "Epoch [542/1000], Train Loss: 0.9954, Train Accuracy: 88.4838, Test Loss: 0.9759, Test Accuracy: 89.9023\n",
      "Epoch [543/1000], Train Loss: 0.9953, Train Accuracy: 88.4838, Test Loss: 0.9758, Test Accuracy: 89.9023\n",
      "Epoch [544/1000], Train Loss: 0.9952, Train Accuracy: 88.4838, Test Loss: 0.9757, Test Accuracy: 89.9023\n",
      "Epoch [545/1000], Train Loss: 0.9950, Train Accuracy: 88.4838, Test Loss: 0.9756, Test Accuracy: 89.9023\n",
      "Epoch [546/1000], Train Loss: 0.9949, Train Accuracy: 88.4838, Test Loss: 0.9755, Test Accuracy: 89.9023\n",
      "Epoch [547/1000], Train Loss: 0.9948, Train Accuracy: 88.4838, Test Loss: 0.9754, Test Accuracy: 89.9023\n",
      "Epoch [548/1000], Train Loss: 0.9947, Train Accuracy: 88.4838, Test Loss: 0.9752, Test Accuracy: 89.9023\n",
      "Epoch [549/1000], Train Loss: 0.9945, Train Accuracy: 88.4838, Test Loss: 0.9751, Test Accuracy: 89.9023\n",
      "Epoch [550/1000], Train Loss: 0.9944, Train Accuracy: 88.4838, Test Loss: 0.9750, Test Accuracy: 89.9023\n",
      "Epoch [551/1000], Train Loss: 0.9943, Train Accuracy: 88.4838, Test Loss: 0.9749, Test Accuracy: 89.9023\n",
      "Epoch [552/1000], Train Loss: 0.9941, Train Accuracy: 88.4838, Test Loss: 0.9748, Test Accuracy: 89.9023\n",
      "Epoch [553/1000], Train Loss: 0.9940, Train Accuracy: 88.4838, Test Loss: 0.9747, Test Accuracy: 89.9023\n",
      "Epoch [554/1000], Train Loss: 0.9939, Train Accuracy: 88.4838, Test Loss: 0.9746, Test Accuracy: 89.9023\n",
      "Epoch [555/1000], Train Loss: 0.9938, Train Accuracy: 88.4838, Test Loss: 0.9745, Test Accuracy: 89.9023\n",
      "Epoch [556/1000], Train Loss: 0.9936, Train Accuracy: 88.4838, Test Loss: 0.9744, Test Accuracy: 89.9023\n",
      "Epoch [557/1000], Train Loss: 0.9935, Train Accuracy: 88.4838, Test Loss: 0.9743, Test Accuracy: 89.9023\n",
      "Epoch [558/1000], Train Loss: 0.9934, Train Accuracy: 88.4838, Test Loss: 0.9742, Test Accuracy: 89.9023\n",
      "Epoch [559/1000], Train Loss: 0.9933, Train Accuracy: 88.4838, Test Loss: 0.9741, Test Accuracy: 89.9023\n",
      "Epoch [560/1000], Train Loss: 0.9932, Train Accuracy: 88.4838, Test Loss: 0.9740, Test Accuracy: 89.9023\n",
      "Epoch [561/1000], Train Loss: 0.9930, Train Accuracy: 88.4838, Test Loss: 0.9739, Test Accuracy: 89.9023\n",
      "Epoch [562/1000], Train Loss: 0.9929, Train Accuracy: 88.4838, Test Loss: 0.9738, Test Accuracy: 89.9023\n",
      "Epoch [563/1000], Train Loss: 0.9928, Train Accuracy: 88.4838, Test Loss: 0.9737, Test Accuracy: 89.9023\n",
      "Epoch [564/1000], Train Loss: 0.9927, Train Accuracy: 88.4838, Test Loss: 0.9736, Test Accuracy: 89.9023\n",
      "Epoch [565/1000], Train Loss: 0.9925, Train Accuracy: 88.4838, Test Loss: 0.9735, Test Accuracy: 89.9023\n",
      "Epoch [566/1000], Train Loss: 0.9924, Train Accuracy: 88.4838, Test Loss: 0.9734, Test Accuracy: 89.9023\n",
      "Epoch [567/1000], Train Loss: 0.9923, Train Accuracy: 88.4838, Test Loss: 0.9733, Test Accuracy: 89.9023\n",
      "Epoch [568/1000], Train Loss: 0.9922, Train Accuracy: 88.4838, Test Loss: 0.9732, Test Accuracy: 89.9023\n",
      "Epoch [569/1000], Train Loss: 0.9921, Train Accuracy: 88.4838, Test Loss: 0.9731, Test Accuracy: 89.9023\n",
      "Epoch [570/1000], Train Loss: 0.9920, Train Accuracy: 88.4838, Test Loss: 0.9730, Test Accuracy: 89.9023\n",
      "Epoch [571/1000], Train Loss: 0.9918, Train Accuracy: 88.4838, Test Loss: 0.9729, Test Accuracy: 89.9023\n",
      "Epoch [572/1000], Train Loss: 0.9917, Train Accuracy: 88.4838, Test Loss: 0.9728, Test Accuracy: 89.9023\n",
      "Epoch [573/1000], Train Loss: 0.9916, Train Accuracy: 88.4838, Test Loss: 0.9727, Test Accuracy: 89.9023\n",
      "Epoch [574/1000], Train Loss: 0.9915, Train Accuracy: 88.4838, Test Loss: 0.9726, Test Accuracy: 89.9023\n",
      "Epoch [575/1000], Train Loss: 0.9914, Train Accuracy: 88.4838, Test Loss: 0.9725, Test Accuracy: 89.9023\n",
      "Epoch [576/1000], Train Loss: 0.9913, Train Accuracy: 88.4838, Test Loss: 0.9724, Test Accuracy: 89.9023\n",
      "Epoch [577/1000], Train Loss: 0.9911, Train Accuracy: 88.4838, Test Loss: 0.9723, Test Accuracy: 89.9023\n",
      "Epoch [578/1000], Train Loss: 0.9910, Train Accuracy: 88.4838, Test Loss: 0.9722, Test Accuracy: 89.9023\n",
      "Epoch [579/1000], Train Loss: 0.9909, Train Accuracy: 88.4838, Test Loss: 0.9721, Test Accuracy: 89.9023\n",
      "Epoch [580/1000], Train Loss: 0.9908, Train Accuracy: 88.4838, Test Loss: 0.9720, Test Accuracy: 89.9023\n",
      "Epoch [581/1000], Train Loss: 0.9907, Train Accuracy: 88.4838, Test Loss: 0.9719, Test Accuracy: 89.9023\n",
      "Epoch [582/1000], Train Loss: 0.9906, Train Accuracy: 88.4838, Test Loss: 0.9718, Test Accuracy: 89.9023\n",
      "Epoch [583/1000], Train Loss: 0.9905, Train Accuracy: 88.4838, Test Loss: 0.9717, Test Accuracy: 89.9023\n",
      "Epoch [584/1000], Train Loss: 0.9904, Train Accuracy: 88.4838, Test Loss: 0.9716, Test Accuracy: 89.9023\n",
      "Epoch [585/1000], Train Loss: 0.9903, Train Accuracy: 88.4838, Test Loss: 0.9716, Test Accuracy: 89.9023\n",
      "Epoch [586/1000], Train Loss: 0.9901, Train Accuracy: 88.4838, Test Loss: 0.9715, Test Accuracy: 89.9023\n",
      "Epoch [587/1000], Train Loss: 0.9900, Train Accuracy: 88.4838, Test Loss: 0.9714, Test Accuracy: 89.9023\n",
      "Epoch [588/1000], Train Loss: 0.9899, Train Accuracy: 88.4838, Test Loss: 0.9713, Test Accuracy: 89.9023\n",
      "Epoch [589/1000], Train Loss: 0.9898, Train Accuracy: 88.4838, Test Loss: 0.9712, Test Accuracy: 89.9023\n",
      "Epoch [590/1000], Train Loss: 0.9897, Train Accuracy: 88.4838, Test Loss: 0.9711, Test Accuracy: 89.9023\n",
      "Epoch [591/1000], Train Loss: 0.9896, Train Accuracy: 88.4838, Test Loss: 0.9710, Test Accuracy: 89.9023\n",
      "Epoch [592/1000], Train Loss: 0.9895, Train Accuracy: 88.4838, Test Loss: 0.9709, Test Accuracy: 89.9023\n",
      "Epoch [593/1000], Train Loss: 0.9894, Train Accuracy: 88.4838, Test Loss: 0.9708, Test Accuracy: 89.9023\n",
      "Epoch [594/1000], Train Loss: 0.9893, Train Accuracy: 88.4838, Test Loss: 0.9708, Test Accuracy: 89.9023\n",
      "Epoch [595/1000], Train Loss: 0.9892, Train Accuracy: 88.4838, Test Loss: 0.9707, Test Accuracy: 89.9023\n",
      "Epoch [596/1000], Train Loss: 0.9891, Train Accuracy: 88.4838, Test Loss: 0.9706, Test Accuracy: 89.9023\n",
      "Epoch [597/1000], Train Loss: 0.9890, Train Accuracy: 88.4838, Test Loss: 0.9705, Test Accuracy: 89.9023\n",
      "Epoch [598/1000], Train Loss: 0.9889, Train Accuracy: 88.4838, Test Loss: 0.9704, Test Accuracy: 89.9023\n",
      "Epoch [599/1000], Train Loss: 0.9888, Train Accuracy: 88.4838, Test Loss: 0.9703, Test Accuracy: 89.9023\n",
      "Epoch [600/1000], Train Loss: 0.9887, Train Accuracy: 88.4838, Test Loss: 0.9702, Test Accuracy: 89.9023\n",
      "Epoch [601/1000], Train Loss: 0.9886, Train Accuracy: 88.4838, Test Loss: 0.9702, Test Accuracy: 89.9023\n",
      "Epoch [602/1000], Train Loss: 0.9885, Train Accuracy: 88.4838, Test Loss: 0.9701, Test Accuracy: 89.9023\n",
      "Epoch [603/1000], Train Loss: 0.9884, Train Accuracy: 88.4838, Test Loss: 0.9700, Test Accuracy: 89.9023\n",
      "Epoch [604/1000], Train Loss: 0.9883, Train Accuracy: 88.4838, Test Loss: 0.9699, Test Accuracy: 89.9023\n",
      "Epoch [605/1000], Train Loss: 0.9882, Train Accuracy: 88.4838, Test Loss: 0.9698, Test Accuracy: 89.9023\n",
      "Epoch [606/1000], Train Loss: 0.9881, Train Accuracy: 88.4838, Test Loss: 0.9698, Test Accuracy: 89.9023\n",
      "Epoch [607/1000], Train Loss: 0.9880, Train Accuracy: 88.4838, Test Loss: 0.9697, Test Accuracy: 89.9023\n",
      "Epoch [608/1000], Train Loss: 0.9879, Train Accuracy: 88.4838, Test Loss: 0.9696, Test Accuracy: 89.9023\n",
      "Epoch [609/1000], Train Loss: 0.9878, Train Accuracy: 88.4838, Test Loss: 0.9695, Test Accuracy: 89.9023\n",
      "Epoch [610/1000], Train Loss: 0.9877, Train Accuracy: 88.4838, Test Loss: 0.9694, Test Accuracy: 89.9023\n",
      "Epoch [611/1000], Train Loss: 0.9876, Train Accuracy: 88.4838, Test Loss: 0.9694, Test Accuracy: 89.9023\n",
      "Epoch [612/1000], Train Loss: 0.9875, Train Accuracy: 88.4838, Test Loss: 0.9693, Test Accuracy: 89.9023\n",
      "Epoch [613/1000], Train Loss: 0.9874, Train Accuracy: 88.4838, Test Loss: 0.9692, Test Accuracy: 89.9023\n",
      "Epoch [614/1000], Train Loss: 0.9873, Train Accuracy: 88.4838, Test Loss: 0.9691, Test Accuracy: 89.9023\n",
      "Epoch [615/1000], Train Loss: 0.9872, Train Accuracy: 88.4838, Test Loss: 0.9690, Test Accuracy: 89.9023\n",
      "Epoch [616/1000], Train Loss: 0.9871, Train Accuracy: 88.4838, Test Loss: 0.9690, Test Accuracy: 89.9023\n",
      "Epoch [617/1000], Train Loss: 0.9870, Train Accuracy: 88.4838, Test Loss: 0.9689, Test Accuracy: 89.9023\n",
      "Epoch [618/1000], Train Loss: 0.9869, Train Accuracy: 88.4838, Test Loss: 0.9688, Test Accuracy: 89.9023\n",
      "Epoch [619/1000], Train Loss: 0.9868, Train Accuracy: 88.4838, Test Loss: 0.9687, Test Accuracy: 89.9023\n",
      "Epoch [620/1000], Train Loss: 0.9867, Train Accuracy: 88.4838, Test Loss: 0.9687, Test Accuracy: 89.9023\n",
      "Epoch [621/1000], Train Loss: 0.9866, Train Accuracy: 88.4838, Test Loss: 0.9686, Test Accuracy: 89.9023\n",
      "Epoch [622/1000], Train Loss: 0.9865, Train Accuracy: 88.4838, Test Loss: 0.9685, Test Accuracy: 89.9023\n",
      "Epoch [623/1000], Train Loss: 0.9864, Train Accuracy: 88.4838, Test Loss: 0.9684, Test Accuracy: 89.9023\n",
      "Epoch [624/1000], Train Loss: 0.9863, Train Accuracy: 88.4838, Test Loss: 0.9684, Test Accuracy: 89.9023\n",
      "Epoch [625/1000], Train Loss: 0.9862, Train Accuracy: 88.4838, Test Loss: 0.9683, Test Accuracy: 89.9023\n",
      "Epoch [626/1000], Train Loss: 0.9862, Train Accuracy: 88.4838, Test Loss: 0.9682, Test Accuracy: 89.9023\n",
      "Epoch [627/1000], Train Loss: 0.9861, Train Accuracy: 88.4838, Test Loss: 0.9681, Test Accuracy: 89.9023\n",
      "Epoch [628/1000], Train Loss: 0.9860, Train Accuracy: 88.4838, Test Loss: 0.9681, Test Accuracy: 89.9023\n",
      "Epoch [629/1000], Train Loss: 0.9859, Train Accuracy: 88.4838, Test Loss: 0.9680, Test Accuracy: 89.9023\n",
      "Epoch [630/1000], Train Loss: 0.9858, Train Accuracy: 88.4838, Test Loss: 0.9679, Test Accuracy: 89.9023\n",
      "Epoch [631/1000], Train Loss: 0.9857, Train Accuracy: 88.4838, Test Loss: 0.9679, Test Accuracy: 89.9023\n",
      "Epoch [632/1000], Train Loss: 0.9856, Train Accuracy: 88.4838, Test Loss: 0.9678, Test Accuracy: 89.9023\n",
      "Epoch [633/1000], Train Loss: 0.9855, Train Accuracy: 88.4838, Test Loss: 0.9677, Test Accuracy: 89.9023\n",
      "Epoch [634/1000], Train Loss: 0.9854, Train Accuracy: 88.4838, Test Loss: 0.9676, Test Accuracy: 89.9023\n",
      "Epoch [635/1000], Train Loss: 0.9853, Train Accuracy: 88.4838, Test Loss: 0.9676, Test Accuracy: 89.9023\n",
      "Epoch [636/1000], Train Loss: 0.9853, Train Accuracy: 88.4838, Test Loss: 0.9675, Test Accuracy: 89.9023\n",
      "Epoch [637/1000], Train Loss: 0.9852, Train Accuracy: 88.4838, Test Loss: 0.9674, Test Accuracy: 89.9023\n",
      "Epoch [638/1000], Train Loss: 0.9851, Train Accuracy: 88.4838, Test Loss: 0.9674, Test Accuracy: 89.9023\n",
      "Epoch [639/1000], Train Loss: 0.9850, Train Accuracy: 88.4838, Test Loss: 0.9673, Test Accuracy: 89.9023\n",
      "Epoch [640/1000], Train Loss: 0.9849, Train Accuracy: 88.4838, Test Loss: 0.9672, Test Accuracy: 89.9023\n",
      "Epoch [641/1000], Train Loss: 0.9848, Train Accuracy: 88.4838, Test Loss: 0.9672, Test Accuracy: 89.9023\n",
      "Epoch [642/1000], Train Loss: 0.9847, Train Accuracy: 88.4838, Test Loss: 0.9671, Test Accuracy: 89.9023\n",
      "Epoch [643/1000], Train Loss: 0.9847, Train Accuracy: 88.4838, Test Loss: 0.9670, Test Accuracy: 89.9023\n",
      "Epoch [644/1000], Train Loss: 0.9846, Train Accuracy: 88.4838, Test Loss: 0.9670, Test Accuracy: 89.9023\n",
      "Epoch [645/1000], Train Loss: 0.9845, Train Accuracy: 88.4838, Test Loss: 0.9669, Test Accuracy: 89.9023\n",
      "Epoch [646/1000], Train Loss: 0.9844, Train Accuracy: 88.4838, Test Loss: 0.9668, Test Accuracy: 89.9023\n",
      "Epoch [647/1000], Train Loss: 0.9843, Train Accuracy: 88.4838, Test Loss: 0.9668, Test Accuracy: 89.9023\n",
      "Epoch [648/1000], Train Loss: 0.9842, Train Accuracy: 88.4838, Test Loss: 0.9667, Test Accuracy: 89.9023\n",
      "Epoch [649/1000], Train Loss: 0.9842, Train Accuracy: 88.4838, Test Loss: 0.9666, Test Accuracy: 89.9023\n",
      "Epoch [650/1000], Train Loss: 0.9841, Train Accuracy: 88.4838, Test Loss: 0.9666, Test Accuracy: 89.9023\n",
      "Epoch [651/1000], Train Loss: 0.9840, Train Accuracy: 88.4838, Test Loss: 0.9665, Test Accuracy: 89.9023\n",
      "Epoch [652/1000], Train Loss: 0.9839, Train Accuracy: 88.4838, Test Loss: 0.9664, Test Accuracy: 89.9023\n",
      "Epoch [653/1000], Train Loss: 0.9838, Train Accuracy: 88.4838, Test Loss: 0.9664, Test Accuracy: 89.9023\n",
      "Epoch [654/1000], Train Loss: 0.9838, Train Accuracy: 88.4838, Test Loss: 0.9663, Test Accuracy: 89.9023\n",
      "Epoch [655/1000], Train Loss: 0.9837, Train Accuracy: 88.4838, Test Loss: 0.9662, Test Accuracy: 89.9023\n",
      "Epoch [656/1000], Train Loss: 0.9836, Train Accuracy: 88.4838, Test Loss: 0.9662, Test Accuracy: 89.9023\n",
      "Epoch [657/1000], Train Loss: 0.9835, Train Accuracy: 88.4838, Test Loss: 0.9661, Test Accuracy: 89.9023\n",
      "Epoch [658/1000], Train Loss: 0.9834, Train Accuracy: 88.4838, Test Loss: 0.9661, Test Accuracy: 89.9023\n",
      "Epoch [659/1000], Train Loss: 0.9834, Train Accuracy: 88.4838, Test Loss: 0.9660, Test Accuracy: 89.9023\n",
      "Epoch [660/1000], Train Loss: 0.9833, Train Accuracy: 88.4838, Test Loss: 0.9659, Test Accuracy: 89.9023\n",
      "Epoch [661/1000], Train Loss: 0.9832, Train Accuracy: 88.4838, Test Loss: 0.9659, Test Accuracy: 89.9023\n",
      "Epoch [662/1000], Train Loss: 0.9831, Train Accuracy: 88.4838, Test Loss: 0.9658, Test Accuracy: 89.9023\n",
      "Epoch [663/1000], Train Loss: 0.9830, Train Accuracy: 88.4838, Test Loss: 0.9657, Test Accuracy: 89.9023\n",
      "Epoch [664/1000], Train Loss: 0.9830, Train Accuracy: 88.4838, Test Loss: 0.9657, Test Accuracy: 89.9023\n",
      "Epoch [665/1000], Train Loss: 0.9829, Train Accuracy: 88.4838, Test Loss: 0.9656, Test Accuracy: 89.9023\n",
      "Epoch [666/1000], Train Loss: 0.9828, Train Accuracy: 88.4838, Test Loss: 0.9656, Test Accuracy: 89.9023\n",
      "Epoch [667/1000], Train Loss: 0.9827, Train Accuracy: 88.4838, Test Loss: 0.9655, Test Accuracy: 89.9023\n",
      "Epoch [668/1000], Train Loss: 0.9827, Train Accuracy: 88.4838, Test Loss: 0.9654, Test Accuracy: 89.9023\n",
      "Epoch [669/1000], Train Loss: 0.9826, Train Accuracy: 88.4838, Test Loss: 0.9654, Test Accuracy: 89.9023\n",
      "Epoch [670/1000], Train Loss: 0.9825, Train Accuracy: 88.4838, Test Loss: 0.9653, Test Accuracy: 89.9023\n",
      "Epoch [671/1000], Train Loss: 0.9824, Train Accuracy: 88.4838, Test Loss: 0.9653, Test Accuracy: 89.9023\n",
      "Epoch [672/1000], Train Loss: 0.9824, Train Accuracy: 88.4838, Test Loss: 0.9652, Test Accuracy: 89.9023\n",
      "Epoch [673/1000], Train Loss: 0.9823, Train Accuracy: 88.4838, Test Loss: 0.9651, Test Accuracy: 89.9023\n",
      "Epoch [674/1000], Train Loss: 0.9822, Train Accuracy: 88.4838, Test Loss: 0.9651, Test Accuracy: 89.9023\n",
      "Epoch [675/1000], Train Loss: 0.9822, Train Accuracy: 88.4838, Test Loss: 0.9650, Test Accuracy: 89.9023\n",
      "Epoch [676/1000], Train Loss: 0.9821, Train Accuracy: 88.4838, Test Loss: 0.9650, Test Accuracy: 89.9023\n",
      "Epoch [677/1000], Train Loss: 0.9820, Train Accuracy: 88.4838, Test Loss: 0.9649, Test Accuracy: 89.9023\n",
      "Epoch [678/1000], Train Loss: 0.9819, Train Accuracy: 88.4838, Test Loss: 0.9649, Test Accuracy: 89.9023\n",
      "Epoch [679/1000], Train Loss: 0.9819, Train Accuracy: 88.4838, Test Loss: 0.9648, Test Accuracy: 89.9023\n",
      "Epoch [680/1000], Train Loss: 0.9818, Train Accuracy: 88.4838, Test Loss: 0.9647, Test Accuracy: 89.9023\n",
      "Epoch [681/1000], Train Loss: 0.9817, Train Accuracy: 88.4838, Test Loss: 0.9647, Test Accuracy: 89.9023\n",
      "Epoch [682/1000], Train Loss: 0.9817, Train Accuracy: 88.4838, Test Loss: 0.9646, Test Accuracy: 89.9023\n",
      "Epoch [683/1000], Train Loss: 0.9816, Train Accuracy: 88.4838, Test Loss: 0.9646, Test Accuracy: 89.9023\n",
      "Epoch [684/1000], Train Loss: 0.9815, Train Accuracy: 88.4838, Test Loss: 0.9645, Test Accuracy: 89.9023\n",
      "Epoch [685/1000], Train Loss: 0.9814, Train Accuracy: 88.4838, Test Loss: 0.9645, Test Accuracy: 89.9023\n",
      "Epoch [686/1000], Train Loss: 0.9814, Train Accuracy: 88.4838, Test Loss: 0.9644, Test Accuracy: 89.9023\n",
      "Epoch [687/1000], Train Loss: 0.9813, Train Accuracy: 88.4838, Test Loss: 0.9644, Test Accuracy: 89.9023\n",
      "Epoch [688/1000], Train Loss: 0.9812, Train Accuracy: 88.4838, Test Loss: 0.9643, Test Accuracy: 89.9023\n",
      "Epoch [689/1000], Train Loss: 0.9812, Train Accuracy: 88.4838, Test Loss: 0.9642, Test Accuracy: 89.9023\n",
      "Epoch [690/1000], Train Loss: 0.9811, Train Accuracy: 88.4838, Test Loss: 0.9642, Test Accuracy: 89.9023\n",
      "Epoch [691/1000], Train Loss: 0.9810, Train Accuracy: 88.4838, Test Loss: 0.9641, Test Accuracy: 89.9023\n",
      "Epoch [692/1000], Train Loss: 0.9810, Train Accuracy: 88.4838, Test Loss: 0.9641, Test Accuracy: 89.9023\n",
      "Epoch [693/1000], Train Loss: 0.9809, Train Accuracy: 88.4838, Test Loss: 0.9640, Test Accuracy: 89.9023\n",
      "Epoch [694/1000], Train Loss: 0.9808, Train Accuracy: 88.4838, Test Loss: 0.9640, Test Accuracy: 89.9023\n",
      "Epoch [695/1000], Train Loss: 0.9808, Train Accuracy: 88.4838, Test Loss: 0.9639, Test Accuracy: 89.9023\n",
      "Epoch [696/1000], Train Loss: 0.9807, Train Accuracy: 88.4838, Test Loss: 0.9639, Test Accuracy: 89.9023\n",
      "Epoch [697/1000], Train Loss: 0.9806, Train Accuracy: 88.4838, Test Loss: 0.9638, Test Accuracy: 89.9023\n",
      "Epoch [698/1000], Train Loss: 0.9806, Train Accuracy: 88.4838, Test Loss: 0.9638, Test Accuracy: 89.9023\n",
      "Epoch [699/1000], Train Loss: 0.9805, Train Accuracy: 88.4838, Test Loss: 0.9637, Test Accuracy: 89.9023\n",
      "Epoch [700/1000], Train Loss: 0.9804, Train Accuracy: 88.4838, Test Loss: 0.9637, Test Accuracy: 89.9023\n",
      "Epoch [701/1000], Train Loss: 0.9804, Train Accuracy: 88.4838, Test Loss: 0.9636, Test Accuracy: 89.9023\n",
      "Epoch [702/1000], Train Loss: 0.9803, Train Accuracy: 88.4838, Test Loss: 0.9636, Test Accuracy: 89.9023\n",
      "Epoch [703/1000], Train Loss: 0.9802, Train Accuracy: 88.4838, Test Loss: 0.9635, Test Accuracy: 89.9023\n",
      "Epoch [704/1000], Train Loss: 0.9802, Train Accuracy: 88.4838, Test Loss: 0.9635, Test Accuracy: 89.9023\n",
      "Epoch [705/1000], Train Loss: 0.9801, Train Accuracy: 88.4838, Test Loss: 0.9634, Test Accuracy: 89.9023\n",
      "Epoch [706/1000], Train Loss: 0.9801, Train Accuracy: 88.4838, Test Loss: 0.9634, Test Accuracy: 89.9023\n",
      "Epoch [707/1000], Train Loss: 0.9800, Train Accuracy: 88.4838, Test Loss: 0.9633, Test Accuracy: 89.9023\n",
      "Epoch [708/1000], Train Loss: 0.9799, Train Accuracy: 88.4838, Test Loss: 0.9633, Test Accuracy: 89.9023\n",
      "Epoch [709/1000], Train Loss: 0.9799, Train Accuracy: 88.4838, Test Loss: 0.9632, Test Accuracy: 89.9023\n",
      "Epoch [710/1000], Train Loss: 0.9798, Train Accuracy: 88.4838, Test Loss: 0.9632, Test Accuracy: 89.9023\n",
      "Epoch [711/1000], Train Loss: 0.9797, Train Accuracy: 88.4838, Test Loss: 0.9631, Test Accuracy: 89.9023\n",
      "Epoch [712/1000], Train Loss: 0.9797, Train Accuracy: 88.4838, Test Loss: 0.9631, Test Accuracy: 89.9023\n",
      "Epoch [713/1000], Train Loss: 0.9796, Train Accuracy: 88.4838, Test Loss: 0.9630, Test Accuracy: 89.9023\n",
      "Epoch [714/1000], Train Loss: 0.9796, Train Accuracy: 88.4838, Test Loss: 0.9630, Test Accuracy: 89.9023\n",
      "Epoch [715/1000], Train Loss: 0.9795, Train Accuracy: 88.4838, Test Loss: 0.9629, Test Accuracy: 89.9023\n",
      "Epoch [716/1000], Train Loss: 0.9794, Train Accuracy: 88.4838, Test Loss: 0.9629, Test Accuracy: 89.9023\n",
      "Epoch [717/1000], Train Loss: 0.9794, Train Accuracy: 88.4838, Test Loss: 0.9628, Test Accuracy: 89.9023\n",
      "Epoch [718/1000], Train Loss: 0.9793, Train Accuracy: 88.4838, Test Loss: 0.9628, Test Accuracy: 89.9023\n",
      "Epoch [719/1000], Train Loss: 0.9793, Train Accuracy: 88.4838, Test Loss: 0.9627, Test Accuracy: 89.9023\n",
      "Epoch [720/1000], Train Loss: 0.9792, Train Accuracy: 88.4838, Test Loss: 0.9627, Test Accuracy: 89.9023\n",
      "Epoch [721/1000], Train Loss: 0.9791, Train Accuracy: 88.4838, Test Loss: 0.9626, Test Accuracy: 89.9023\n",
      "Epoch [722/1000], Train Loss: 0.9791, Train Accuracy: 88.4838, Test Loss: 0.9626, Test Accuracy: 89.9023\n",
      "Epoch [723/1000], Train Loss: 0.9790, Train Accuracy: 88.4838, Test Loss: 0.9626, Test Accuracy: 89.9023\n",
      "Epoch [724/1000], Train Loss: 0.9790, Train Accuracy: 88.4838, Test Loss: 0.9625, Test Accuracy: 89.9023\n",
      "Epoch [725/1000], Train Loss: 0.9789, Train Accuracy: 88.4838, Test Loss: 0.9625, Test Accuracy: 89.9023\n",
      "Epoch [726/1000], Train Loss: 0.9789, Train Accuracy: 88.4838, Test Loss: 0.9624, Test Accuracy: 89.9023\n",
      "Epoch [727/1000], Train Loss: 0.9788, Train Accuracy: 88.4838, Test Loss: 0.9624, Test Accuracy: 89.9023\n",
      "Epoch [728/1000], Train Loss: 0.9787, Train Accuracy: 88.4838, Test Loss: 0.9623, Test Accuracy: 89.9023\n",
      "Epoch [729/1000], Train Loss: 0.9787, Train Accuracy: 88.4838, Test Loss: 0.9623, Test Accuracy: 89.9023\n",
      "Epoch [730/1000], Train Loss: 0.9786, Train Accuracy: 88.4838, Test Loss: 0.9622, Test Accuracy: 89.9023\n",
      "Epoch [731/1000], Train Loss: 0.9786, Train Accuracy: 88.4838, Test Loss: 0.9622, Test Accuracy: 89.9023\n",
      "Epoch [732/1000], Train Loss: 0.9785, Train Accuracy: 88.4838, Test Loss: 0.9622, Test Accuracy: 89.9023\n",
      "Epoch [733/1000], Train Loss: 0.9785, Train Accuracy: 88.4838, Test Loss: 0.9621, Test Accuracy: 89.9023\n",
      "Epoch [734/1000], Train Loss: 0.9784, Train Accuracy: 88.4838, Test Loss: 0.9621, Test Accuracy: 89.9023\n",
      "Epoch [735/1000], Train Loss: 0.9784, Train Accuracy: 88.4838, Test Loss: 0.9620, Test Accuracy: 89.9023\n",
      "Epoch [736/1000], Train Loss: 0.9783, Train Accuracy: 88.4838, Test Loss: 0.9620, Test Accuracy: 89.9023\n",
      "Epoch [737/1000], Train Loss: 0.9783, Train Accuracy: 88.4838, Test Loss: 0.9619, Test Accuracy: 89.9023\n",
      "Epoch [738/1000], Train Loss: 0.9782, Train Accuracy: 88.4838, Test Loss: 0.9619, Test Accuracy: 89.9023\n",
      "Epoch [739/1000], Train Loss: 0.9781, Train Accuracy: 88.4838, Test Loss: 0.9619, Test Accuracy: 89.9023\n",
      "Epoch [740/1000], Train Loss: 0.9781, Train Accuracy: 88.4838, Test Loss: 0.9618, Test Accuracy: 89.9023\n",
      "Epoch [741/1000], Train Loss: 0.9780, Train Accuracy: 88.4838, Test Loss: 0.9618, Test Accuracy: 89.9023\n",
      "Epoch [742/1000], Train Loss: 0.9780, Train Accuracy: 88.4838, Test Loss: 0.9617, Test Accuracy: 89.9023\n",
      "Epoch [743/1000], Train Loss: 0.9779, Train Accuracy: 88.4838, Test Loss: 0.9617, Test Accuracy: 89.9023\n",
      "Epoch [744/1000], Train Loss: 0.9779, Train Accuracy: 88.4838, Test Loss: 0.9617, Test Accuracy: 89.9023\n",
      "Epoch [745/1000], Train Loss: 0.9778, Train Accuracy: 88.4838, Test Loss: 0.9616, Test Accuracy: 89.9023\n",
      "Epoch [746/1000], Train Loss: 0.9778, Train Accuracy: 88.4838, Test Loss: 0.9616, Test Accuracy: 89.9023\n",
      "Epoch [747/1000], Train Loss: 0.9777, Train Accuracy: 88.4838, Test Loss: 0.9615, Test Accuracy: 89.9023\n",
      "Epoch [748/1000], Train Loss: 0.9777, Train Accuracy: 88.4838, Test Loss: 0.9615, Test Accuracy: 89.9023\n",
      "Epoch [749/1000], Train Loss: 0.9776, Train Accuracy: 88.4838, Test Loss: 0.9615, Test Accuracy: 89.9023\n",
      "Epoch [750/1000], Train Loss: 0.9776, Train Accuracy: 88.4838, Test Loss: 0.9614, Test Accuracy: 89.9023\n",
      "Epoch [751/1000], Train Loss: 0.9775, Train Accuracy: 88.4838, Test Loss: 0.9614, Test Accuracy: 89.9023\n",
      "Epoch [752/1000], Train Loss: 0.9775, Train Accuracy: 88.4838, Test Loss: 0.9613, Test Accuracy: 89.9023\n",
      "Epoch [753/1000], Train Loss: 0.9774, Train Accuracy: 88.4838, Test Loss: 0.9613, Test Accuracy: 89.9023\n",
      "Epoch [754/1000], Train Loss: 0.9774, Train Accuracy: 88.4838, Test Loss: 0.9613, Test Accuracy: 89.9023\n",
      "Epoch [755/1000], Train Loss: 0.9773, Train Accuracy: 88.4838, Test Loss: 0.9612, Test Accuracy: 89.9023\n",
      "Epoch [756/1000], Train Loss: 0.9773, Train Accuracy: 88.4838, Test Loss: 0.9612, Test Accuracy: 89.9023\n",
      "Epoch [757/1000], Train Loss: 0.9772, Train Accuracy: 88.4838, Test Loss: 0.9611, Test Accuracy: 89.9023\n",
      "Epoch [758/1000], Train Loss: 0.9772, Train Accuracy: 88.4838, Test Loss: 0.9611, Test Accuracy: 89.9023\n",
      "Epoch [759/1000], Train Loss: 0.9771, Train Accuracy: 88.4838, Test Loss: 0.9611, Test Accuracy: 89.9023\n",
      "Epoch [760/1000], Train Loss: 0.9771, Train Accuracy: 88.4838, Test Loss: 0.9610, Test Accuracy: 89.9023\n",
      "Epoch [761/1000], Train Loss: 0.9770, Train Accuracy: 88.4838, Test Loss: 0.9610, Test Accuracy: 89.9023\n",
      "Epoch [762/1000], Train Loss: 0.9770, Train Accuracy: 88.4838, Test Loss: 0.9609, Test Accuracy: 89.9023\n",
      "Epoch [763/1000], Train Loss: 0.9769, Train Accuracy: 88.4838, Test Loss: 0.9609, Test Accuracy: 89.9023\n",
      "Epoch [764/1000], Train Loss: 0.9769, Train Accuracy: 88.4838, Test Loss: 0.9609, Test Accuracy: 89.9023\n",
      "Epoch [765/1000], Train Loss: 0.9769, Train Accuracy: 88.4838, Test Loss: 0.9608, Test Accuracy: 89.9023\n",
      "Epoch [766/1000], Train Loss: 0.9768, Train Accuracy: 88.4838, Test Loss: 0.9608, Test Accuracy: 89.9023\n",
      "Epoch [767/1000], Train Loss: 0.9768, Train Accuracy: 88.4838, Test Loss: 0.9608, Test Accuracy: 89.9023\n",
      "Epoch [768/1000], Train Loss: 0.9767, Train Accuracy: 88.4838, Test Loss: 0.9607, Test Accuracy: 89.9023\n",
      "Epoch [769/1000], Train Loss: 0.9767, Train Accuracy: 88.4838, Test Loss: 0.9607, Test Accuracy: 89.9023\n",
      "Epoch [770/1000], Train Loss: 0.9766, Train Accuracy: 88.4838, Test Loss: 0.9607, Test Accuracy: 89.9023\n",
      "Epoch [771/1000], Train Loss: 0.9766, Train Accuracy: 88.4838, Test Loss: 0.9606, Test Accuracy: 89.9023\n",
      "Epoch [772/1000], Train Loss: 0.9765, Train Accuracy: 88.4838, Test Loss: 0.9606, Test Accuracy: 89.9023\n",
      "Epoch [773/1000], Train Loss: 0.9765, Train Accuracy: 88.4838, Test Loss: 0.9606, Test Accuracy: 89.9023\n",
      "Epoch [774/1000], Train Loss: 0.9764, Train Accuracy: 88.4838, Test Loss: 0.9605, Test Accuracy: 89.9023\n",
      "Epoch [775/1000], Train Loss: 0.9764, Train Accuracy: 88.4838, Test Loss: 0.9605, Test Accuracy: 89.9023\n",
      "Epoch [776/1000], Train Loss: 0.9764, Train Accuracy: 88.4838, Test Loss: 0.9604, Test Accuracy: 89.9023\n",
      "Epoch [777/1000], Train Loss: 0.9763, Train Accuracy: 88.4838, Test Loss: 0.9604, Test Accuracy: 89.9023\n",
      "Epoch [778/1000], Train Loss: 0.9763, Train Accuracy: 88.4838, Test Loss: 0.9604, Test Accuracy: 89.9023\n",
      "Epoch [779/1000], Train Loss: 0.9762, Train Accuracy: 88.4838, Test Loss: 0.9603, Test Accuracy: 89.9023\n",
      "Epoch [780/1000], Train Loss: 0.9762, Train Accuracy: 88.4838, Test Loss: 0.9603, Test Accuracy: 89.9023\n",
      "Epoch [781/1000], Train Loss: 0.9761, Train Accuracy: 88.4838, Test Loss: 0.9603, Test Accuracy: 89.9023\n",
      "Epoch [782/1000], Train Loss: 0.9761, Train Accuracy: 88.4838, Test Loss: 0.9603, Test Accuracy: 89.9023\n",
      "Epoch [783/1000], Train Loss: 0.9761, Train Accuracy: 88.4838, Test Loss: 0.9602, Test Accuracy: 89.9023\n",
      "Epoch [784/1000], Train Loss: 0.9760, Train Accuracy: 88.4838, Test Loss: 0.9602, Test Accuracy: 89.9023\n",
      "Epoch [785/1000], Train Loss: 0.9760, Train Accuracy: 88.4838, Test Loss: 0.9602, Test Accuracy: 89.9023\n",
      "Epoch [786/1000], Train Loss: 0.9759, Train Accuracy: 88.4838, Test Loss: 0.9601, Test Accuracy: 89.9023\n",
      "Epoch [787/1000], Train Loss: 0.9759, Train Accuracy: 88.4838, Test Loss: 0.9601, Test Accuracy: 89.9023\n",
      "Epoch [788/1000], Train Loss: 0.9758, Train Accuracy: 88.4838, Test Loss: 0.9601, Test Accuracy: 89.9023\n",
      "Epoch [789/1000], Train Loss: 0.9758, Train Accuracy: 88.4838, Test Loss: 0.9600, Test Accuracy: 89.9023\n",
      "Epoch [790/1000], Train Loss: 0.9758, Train Accuracy: 88.4838, Test Loss: 0.9600, Test Accuracy: 89.9023\n",
      "Epoch [791/1000], Train Loss: 0.9757, Train Accuracy: 88.4838, Test Loss: 0.9600, Test Accuracy: 89.9023\n",
      "Epoch [792/1000], Train Loss: 0.9757, Train Accuracy: 88.4838, Test Loss: 0.9599, Test Accuracy: 89.9023\n",
      "Epoch [793/1000], Train Loss: 0.9756, Train Accuracy: 88.4838, Test Loss: 0.9599, Test Accuracy: 89.9023\n",
      "Epoch [794/1000], Train Loss: 0.9756, Train Accuracy: 88.4838, Test Loss: 0.9599, Test Accuracy: 89.9023\n",
      "Epoch [795/1000], Train Loss: 0.9756, Train Accuracy: 88.4838, Test Loss: 0.9598, Test Accuracy: 89.9023\n",
      "Epoch [796/1000], Train Loss: 0.9755, Train Accuracy: 88.4838, Test Loss: 0.9598, Test Accuracy: 89.9023\n",
      "Epoch [797/1000], Train Loss: 0.9755, Train Accuracy: 88.4838, Test Loss: 0.9598, Test Accuracy: 89.9023\n",
      "Epoch [798/1000], Train Loss: 0.9754, Train Accuracy: 88.4838, Test Loss: 0.9597, Test Accuracy: 89.9023\n",
      "Epoch [799/1000], Train Loss: 0.9754, Train Accuracy: 88.4838, Test Loss: 0.9597, Test Accuracy: 89.9023\n",
      "Epoch [800/1000], Train Loss: 0.9754, Train Accuracy: 88.4838, Test Loss: 0.9597, Test Accuracy: 89.9023\n",
      "Epoch [801/1000], Train Loss: 0.9753, Train Accuracy: 88.4838, Test Loss: 0.9596, Test Accuracy: 89.9023\n",
      "Epoch [802/1000], Train Loss: 0.9753, Train Accuracy: 88.4838, Test Loss: 0.9596, Test Accuracy: 89.9023\n",
      "Epoch [803/1000], Train Loss: 0.9752, Train Accuracy: 88.4838, Test Loss: 0.9596, Test Accuracy: 89.9023\n",
      "Epoch [804/1000], Train Loss: 0.9752, Train Accuracy: 88.4838, Test Loss: 0.9595, Test Accuracy: 89.9023\n",
      "Epoch [805/1000], Train Loss: 0.9752, Train Accuracy: 88.4838, Test Loss: 0.9595, Test Accuracy: 89.9023\n",
      "Epoch [806/1000], Train Loss: 0.9751, Train Accuracy: 88.4838, Test Loss: 0.9595, Test Accuracy: 89.9023\n",
      "Epoch [807/1000], Train Loss: 0.9751, Train Accuracy: 88.4838, Test Loss: 0.9594, Test Accuracy: 89.9023\n",
      "Epoch [808/1000], Train Loss: 0.9750, Train Accuracy: 88.4838, Test Loss: 0.9594, Test Accuracy: 89.9023\n",
      "Epoch [809/1000], Train Loss: 0.9750, Train Accuracy: 88.4838, Test Loss: 0.9594, Test Accuracy: 89.9023\n",
      "Epoch [810/1000], Train Loss: 0.9750, Train Accuracy: 88.4838, Test Loss: 0.9594, Test Accuracy: 89.9023\n",
      "Epoch [811/1000], Train Loss: 0.9749, Train Accuracy: 88.4838, Test Loss: 0.9593, Test Accuracy: 89.9023\n",
      "Epoch [812/1000], Train Loss: 0.9749, Train Accuracy: 88.4838, Test Loss: 0.9593, Test Accuracy: 89.9023\n",
      "Epoch [813/1000], Train Loss: 0.9749, Train Accuracy: 88.4838, Test Loss: 0.9593, Test Accuracy: 89.9023\n",
      "Epoch [814/1000], Train Loss: 0.9748, Train Accuracy: 88.4838, Test Loss: 0.9592, Test Accuracy: 89.9023\n",
      "Epoch [815/1000], Train Loss: 0.9748, Train Accuracy: 88.4838, Test Loss: 0.9592, Test Accuracy: 89.9023\n",
      "Epoch [816/1000], Train Loss: 0.9747, Train Accuracy: 88.4838, Test Loss: 0.9592, Test Accuracy: 89.9023\n",
      "Epoch [817/1000], Train Loss: 0.9747, Train Accuracy: 88.4838, Test Loss: 0.9591, Test Accuracy: 89.9023\n",
      "Epoch [818/1000], Train Loss: 0.9747, Train Accuracy: 88.4838, Test Loss: 0.9591, Test Accuracy: 89.9023\n",
      "Epoch [819/1000], Train Loss: 0.9746, Train Accuracy: 88.4838, Test Loss: 0.9591, Test Accuracy: 89.9023\n",
      "Epoch [820/1000], Train Loss: 0.9746, Train Accuracy: 88.4838, Test Loss: 0.9591, Test Accuracy: 89.9023\n",
      "Epoch [821/1000], Train Loss: 0.9746, Train Accuracy: 88.4838, Test Loss: 0.9590, Test Accuracy: 89.9023\n",
      "Epoch [822/1000], Train Loss: 0.9745, Train Accuracy: 88.4838, Test Loss: 0.9590, Test Accuracy: 89.9023\n",
      "Epoch [823/1000], Train Loss: 0.9745, Train Accuracy: 88.4838, Test Loss: 0.9590, Test Accuracy: 89.9023\n",
      "Epoch [824/1000], Train Loss: 0.9745, Train Accuracy: 88.4838, Test Loss: 0.9589, Test Accuracy: 89.9023\n",
      "Epoch [825/1000], Train Loss: 0.9744, Train Accuracy: 88.4838, Test Loss: 0.9589, Test Accuracy: 89.9023\n",
      "Epoch [826/1000], Train Loss: 0.9744, Train Accuracy: 88.4838, Test Loss: 0.9589, Test Accuracy: 89.9023\n",
      "Epoch [827/1000], Train Loss: 0.9744, Train Accuracy: 88.4838, Test Loss: 0.9589, Test Accuracy: 89.9023\n",
      "Epoch [828/1000], Train Loss: 0.9743, Train Accuracy: 88.4838, Test Loss: 0.9588, Test Accuracy: 89.9023\n",
      "Epoch [829/1000], Train Loss: 0.9743, Train Accuracy: 88.4838, Test Loss: 0.9588, Test Accuracy: 89.9023\n",
      "Epoch [830/1000], Train Loss: 0.9743, Train Accuracy: 88.4838, Test Loss: 0.9588, Test Accuracy: 89.9023\n",
      "Epoch [831/1000], Train Loss: 0.9742, Train Accuracy: 88.4838, Test Loss: 0.9588, Test Accuracy: 89.9023\n",
      "Epoch [832/1000], Train Loss: 0.9742, Train Accuracy: 88.4838, Test Loss: 0.9587, Test Accuracy: 89.9023\n",
      "Epoch [833/1000], Train Loss: 0.9741, Train Accuracy: 88.4838, Test Loss: 0.9587, Test Accuracy: 89.9023\n",
      "Epoch [834/1000], Train Loss: 0.9741, Train Accuracy: 88.4838, Test Loss: 0.9587, Test Accuracy: 89.9023\n",
      "Epoch [835/1000], Train Loss: 0.9741, Train Accuracy: 88.4838, Test Loss: 0.9586, Test Accuracy: 89.9023\n",
      "Epoch [836/1000], Train Loss: 0.9740, Train Accuracy: 88.4838, Test Loss: 0.9586, Test Accuracy: 89.9023\n",
      "Epoch [837/1000], Train Loss: 0.9740, Train Accuracy: 88.4838, Test Loss: 0.9586, Test Accuracy: 89.9023\n",
      "Epoch [838/1000], Train Loss: 0.9740, Train Accuracy: 88.4838, Test Loss: 0.9586, Test Accuracy: 89.9023\n",
      "Epoch [839/1000], Train Loss: 0.9739, Train Accuracy: 88.4838, Test Loss: 0.9585, Test Accuracy: 89.9023\n",
      "Epoch [840/1000], Train Loss: 0.9739, Train Accuracy: 88.4838, Test Loss: 0.9585, Test Accuracy: 89.9023\n",
      "Epoch [841/1000], Train Loss: 0.9739, Train Accuracy: 88.4838, Test Loss: 0.9585, Test Accuracy: 89.9023\n",
      "Epoch [842/1000], Train Loss: 0.9739, Train Accuracy: 88.4838, Test Loss: 0.9585, Test Accuracy: 89.9023\n",
      "Epoch [843/1000], Train Loss: 0.9738, Train Accuracy: 88.4838, Test Loss: 0.9584, Test Accuracy: 89.9023\n",
      "Epoch [844/1000], Train Loss: 0.9738, Train Accuracy: 88.4838, Test Loss: 0.9584, Test Accuracy: 89.9023\n",
      "Epoch [845/1000], Train Loss: 0.9738, Train Accuracy: 88.4838, Test Loss: 0.9584, Test Accuracy: 89.9023\n",
      "Epoch [846/1000], Train Loss: 0.9737, Train Accuracy: 88.4838, Test Loss: 0.9584, Test Accuracy: 89.9023\n",
      "Epoch [847/1000], Train Loss: 0.9737, Train Accuracy: 88.4838, Test Loss: 0.9583, Test Accuracy: 89.9023\n",
      "Epoch [848/1000], Train Loss: 0.9737, Train Accuracy: 88.4838, Test Loss: 0.9583, Test Accuracy: 89.9023\n",
      "Epoch [849/1000], Train Loss: 0.9736, Train Accuracy: 88.4838, Test Loss: 0.9583, Test Accuracy: 89.9023\n",
      "Epoch [850/1000], Train Loss: 0.9736, Train Accuracy: 88.4838, Test Loss: 0.9583, Test Accuracy: 89.9023\n",
      "Epoch [851/1000], Train Loss: 0.9736, Train Accuracy: 88.4838, Test Loss: 0.9582, Test Accuracy: 89.9023\n",
      "Epoch [852/1000], Train Loss: 0.9735, Train Accuracy: 88.4838, Test Loss: 0.9582, Test Accuracy: 89.9023\n",
      "Epoch [853/1000], Train Loss: 0.9735, Train Accuracy: 88.4838, Test Loss: 0.9582, Test Accuracy: 89.9023\n",
      "Epoch [854/1000], Train Loss: 0.9735, Train Accuracy: 88.4838, Test Loss: 0.9582, Test Accuracy: 89.9023\n",
      "Epoch [855/1000], Train Loss: 0.9734, Train Accuracy: 88.4838, Test Loss: 0.9581, Test Accuracy: 89.9023\n",
      "Epoch [856/1000], Train Loss: 0.9734, Train Accuracy: 88.4838, Test Loss: 0.9581, Test Accuracy: 89.9023\n",
      "Epoch [857/1000], Train Loss: 0.9734, Train Accuracy: 88.4838, Test Loss: 0.9581, Test Accuracy: 89.9023\n",
      "Epoch [858/1000], Train Loss: 0.9733, Train Accuracy: 88.4838, Test Loss: 0.9581, Test Accuracy: 89.9023\n",
      "Epoch [859/1000], Train Loss: 0.9733, Train Accuracy: 88.4838, Test Loss: 0.9580, Test Accuracy: 89.9023\n",
      "Epoch [860/1000], Train Loss: 0.9733, Train Accuracy: 88.4838, Test Loss: 0.9580, Test Accuracy: 89.9023\n",
      "Epoch [861/1000], Train Loss: 0.9733, Train Accuracy: 88.4838, Test Loss: 0.9580, Test Accuracy: 89.9023\n",
      "Epoch [862/1000], Train Loss: 0.9732, Train Accuracy: 88.4838, Test Loss: 0.9580, Test Accuracy: 89.9023\n",
      "Epoch [863/1000], Train Loss: 0.9732, Train Accuracy: 88.4838, Test Loss: 0.9579, Test Accuracy: 89.9023\n",
      "Epoch [864/1000], Train Loss: 0.9732, Train Accuracy: 88.4838, Test Loss: 0.9579, Test Accuracy: 89.9023\n",
      "Epoch [865/1000], Train Loss: 0.9731, Train Accuracy: 88.4838, Test Loss: 0.9579, Test Accuracy: 89.9023\n",
      "Epoch [866/1000], Train Loss: 0.9731, Train Accuracy: 88.4838, Test Loss: 0.9579, Test Accuracy: 89.9023\n",
      "Epoch [867/1000], Train Loss: 0.9731, Train Accuracy: 88.4838, Test Loss: 0.9578, Test Accuracy: 89.9023\n",
      "Epoch [868/1000], Train Loss: 0.9730, Train Accuracy: 88.4838, Test Loss: 0.9578, Test Accuracy: 89.9023\n",
      "Epoch [869/1000], Train Loss: 0.9730, Train Accuracy: 88.4838, Test Loss: 0.9578, Test Accuracy: 89.9023\n",
      "Epoch [870/1000], Train Loss: 0.9730, Train Accuracy: 88.4838, Test Loss: 0.9578, Test Accuracy: 89.9023\n",
      "Epoch [871/1000], Train Loss: 0.9730, Train Accuracy: 88.4838, Test Loss: 0.9577, Test Accuracy: 89.9023\n",
      "Epoch [872/1000], Train Loss: 0.9729, Train Accuracy: 88.4838, Test Loss: 0.9577, Test Accuracy: 89.9023\n",
      "Epoch [873/1000], Train Loss: 0.9729, Train Accuracy: 88.4838, Test Loss: 0.9577, Test Accuracy: 89.9023\n",
      "Epoch [874/1000], Train Loss: 0.9729, Train Accuracy: 88.4838, Test Loss: 0.9577, Test Accuracy: 89.9023\n",
      "Epoch [875/1000], Train Loss: 0.9728, Train Accuracy: 88.4838, Test Loss: 0.9577, Test Accuracy: 89.9023\n",
      "Epoch [876/1000], Train Loss: 0.9728, Train Accuracy: 88.4838, Test Loss: 0.9576, Test Accuracy: 89.9023\n",
      "Epoch [877/1000], Train Loss: 0.9728, Train Accuracy: 88.4838, Test Loss: 0.9576, Test Accuracy: 89.9023\n",
      "Epoch [878/1000], Train Loss: 0.9728, Train Accuracy: 88.4838, Test Loss: 0.9576, Test Accuracy: 89.9023\n",
      "Epoch [879/1000], Train Loss: 0.9727, Train Accuracy: 88.4838, Test Loss: 0.9576, Test Accuracy: 89.9023\n",
      "Epoch [880/1000], Train Loss: 0.9727, Train Accuracy: 88.4838, Test Loss: 0.9575, Test Accuracy: 89.9023\n",
      "Epoch [881/1000], Train Loss: 0.9727, Train Accuracy: 88.4838, Test Loss: 0.9575, Test Accuracy: 89.9023\n",
      "Epoch [882/1000], Train Loss: 0.9726, Train Accuracy: 88.4838, Test Loss: 0.9575, Test Accuracy: 89.9023\n",
      "Epoch [883/1000], Train Loss: 0.9726, Train Accuracy: 88.4838, Test Loss: 0.9575, Test Accuracy: 89.9023\n",
      "Epoch [884/1000], Train Loss: 0.9726, Train Accuracy: 88.4838, Test Loss: 0.9575, Test Accuracy: 89.9023\n",
      "Epoch [885/1000], Train Loss: 0.9726, Train Accuracy: 88.4838, Test Loss: 0.9574, Test Accuracy: 89.9023\n",
      "Epoch [886/1000], Train Loss: 0.9725, Train Accuracy: 88.4838, Test Loss: 0.9574, Test Accuracy: 89.9023\n",
      "Epoch [887/1000], Train Loss: 0.9725, Train Accuracy: 88.4838, Test Loss: 0.9574, Test Accuracy: 89.9023\n",
      "Epoch [888/1000], Train Loss: 0.9725, Train Accuracy: 88.4838, Test Loss: 0.9574, Test Accuracy: 89.9023\n",
      "Epoch [889/1000], Train Loss: 0.9725, Train Accuracy: 88.4838, Test Loss: 0.9573, Test Accuracy: 89.9023\n",
      "Epoch [890/1000], Train Loss: 0.9724, Train Accuracy: 88.4838, Test Loss: 0.9573, Test Accuracy: 89.9023\n",
      "Epoch [891/1000], Train Loss: 0.9724, Train Accuracy: 88.4838, Test Loss: 0.9573, Test Accuracy: 89.9023\n",
      "Epoch [892/1000], Train Loss: 0.9724, Train Accuracy: 88.4838, Test Loss: 0.9573, Test Accuracy: 89.9023\n",
      "Epoch [893/1000], Train Loss: 0.9724, Train Accuracy: 88.4838, Test Loss: 0.9573, Test Accuracy: 89.9023\n",
      "Epoch [894/1000], Train Loss: 0.9723, Train Accuracy: 88.4838, Test Loss: 0.9572, Test Accuracy: 89.9023\n",
      "Epoch [895/1000], Train Loss: 0.9723, Train Accuracy: 88.4838, Test Loss: 0.9572, Test Accuracy: 89.9023\n",
      "Epoch [896/1000], Train Loss: 0.9723, Train Accuracy: 88.4838, Test Loss: 0.9572, Test Accuracy: 89.9023\n",
      "Epoch [897/1000], Train Loss: 0.9723, Train Accuracy: 88.4838, Test Loss: 0.9572, Test Accuracy: 89.9023\n",
      "Epoch [898/1000], Train Loss: 0.9722, Train Accuracy: 88.4838, Test Loss: 0.9572, Test Accuracy: 89.9023\n",
      "Epoch [899/1000], Train Loss: 0.9722, Train Accuracy: 88.4838, Test Loss: 0.9571, Test Accuracy: 89.9023\n",
      "Epoch [900/1000], Train Loss: 0.9722, Train Accuracy: 88.4838, Test Loss: 0.9571, Test Accuracy: 89.9023\n",
      "Epoch [901/1000], Train Loss: 0.9721, Train Accuracy: 88.4838, Test Loss: 0.9571, Test Accuracy: 89.9023\n",
      "Epoch [902/1000], Train Loss: 0.9721, Train Accuracy: 88.4838, Test Loss: 0.9571, Test Accuracy: 89.9023\n",
      "Epoch [903/1000], Train Loss: 0.9721, Train Accuracy: 88.4838, Test Loss: 0.9571, Test Accuracy: 89.9023\n",
      "Epoch [904/1000], Train Loss: 0.9721, Train Accuracy: 88.4838, Test Loss: 0.9570, Test Accuracy: 89.9023\n",
      "Epoch [905/1000], Train Loss: 0.9720, Train Accuracy: 88.4838, Test Loss: 0.9570, Test Accuracy: 89.9023\n",
      "Epoch [906/1000], Train Loss: 0.9720, Train Accuracy: 88.4838, Test Loss: 0.9570, Test Accuracy: 89.9023\n",
      "Epoch [907/1000], Train Loss: 0.9720, Train Accuracy: 88.4838, Test Loss: 0.9570, Test Accuracy: 89.9023\n",
      "Epoch [908/1000], Train Loss: 0.9720, Train Accuracy: 88.4838, Test Loss: 0.9570, Test Accuracy: 89.9023\n",
      "Epoch [909/1000], Train Loss: 0.9719, Train Accuracy: 88.4838, Test Loss: 0.9569, Test Accuracy: 89.9023\n",
      "Epoch [910/1000], Train Loss: 0.9719, Train Accuracy: 88.4838, Test Loss: 0.9569, Test Accuracy: 89.9023\n",
      "Epoch [911/1000], Train Loss: 0.9719, Train Accuracy: 88.4838, Test Loss: 0.9569, Test Accuracy: 89.9023\n",
      "Epoch [912/1000], Train Loss: 0.9719, Train Accuracy: 88.4838, Test Loss: 0.9569, Test Accuracy: 89.9023\n",
      "Epoch [913/1000], Train Loss: 0.9719, Train Accuracy: 88.4838, Test Loss: 0.9569, Test Accuracy: 89.9023\n",
      "Epoch [914/1000], Train Loss: 0.9718, Train Accuracy: 88.4838, Test Loss: 0.9568, Test Accuracy: 89.9023\n",
      "Epoch [915/1000], Train Loss: 0.9718, Train Accuracy: 88.4838, Test Loss: 0.9568, Test Accuracy: 89.9023\n",
      "Epoch [916/1000], Train Loss: 0.9718, Train Accuracy: 88.4838, Test Loss: 0.9568, Test Accuracy: 89.9023\n",
      "Epoch [917/1000], Train Loss: 0.9718, Train Accuracy: 88.4838, Test Loss: 0.9568, Test Accuracy: 89.9023\n",
      "Epoch [918/1000], Train Loss: 0.9717, Train Accuracy: 88.4838, Test Loss: 0.9568, Test Accuracy: 89.9023\n",
      "Epoch [919/1000], Train Loss: 0.9717, Train Accuracy: 88.4838, Test Loss: 0.9567, Test Accuracy: 89.9023\n",
      "Epoch [920/1000], Train Loss: 0.9717, Train Accuracy: 88.4838, Test Loss: 0.9567, Test Accuracy: 89.9023\n",
      "Epoch [921/1000], Train Loss: 0.9717, Train Accuracy: 88.4838, Test Loss: 0.9567, Test Accuracy: 89.9023\n",
      "Epoch [922/1000], Train Loss: 0.9716, Train Accuracy: 88.4838, Test Loss: 0.9567, Test Accuracy: 89.9023\n",
      "Epoch [923/1000], Train Loss: 0.9716, Train Accuracy: 88.4838, Test Loss: 0.9567, Test Accuracy: 89.9023\n",
      "Epoch [924/1000], Train Loss: 0.9716, Train Accuracy: 88.4838, Test Loss: 0.9566, Test Accuracy: 89.9023\n",
      "Epoch [925/1000], Train Loss: 0.9716, Train Accuracy: 88.4838, Test Loss: 0.9566, Test Accuracy: 89.9023\n",
      "Epoch [926/1000], Train Loss: 0.9715, Train Accuracy: 88.4838, Test Loss: 0.9566, Test Accuracy: 89.9023\n",
      "Epoch [927/1000], Train Loss: 0.9715, Train Accuracy: 88.4838, Test Loss: 0.9566, Test Accuracy: 89.9023\n",
      "Epoch [928/1000], Train Loss: 0.9715, Train Accuracy: 88.4838, Test Loss: 0.9566, Test Accuracy: 89.9023\n",
      "Epoch [929/1000], Train Loss: 0.9715, Train Accuracy: 88.4838, Test Loss: 0.9566, Test Accuracy: 89.9023\n",
      "Epoch [930/1000], Train Loss: 0.9715, Train Accuracy: 88.4838, Test Loss: 0.9565, Test Accuracy: 89.9023\n",
      "Epoch [931/1000], Train Loss: 0.9714, Train Accuracy: 88.4838, Test Loss: 0.9565, Test Accuracy: 89.9023\n",
      "Epoch [932/1000], Train Loss: 0.9714, Train Accuracy: 88.4838, Test Loss: 0.9565, Test Accuracy: 89.9023\n",
      "Epoch [933/1000], Train Loss: 0.9714, Train Accuracy: 88.4838, Test Loss: 0.9565, Test Accuracy: 89.9023\n",
      "Epoch [934/1000], Train Loss: 0.9714, Train Accuracy: 88.4838, Test Loss: 0.9565, Test Accuracy: 89.9023\n",
      "Epoch [935/1000], Train Loss: 0.9713, Train Accuracy: 88.4838, Test Loss: 0.9564, Test Accuracy: 89.9023\n",
      "Epoch [936/1000], Train Loss: 0.9713, Train Accuracy: 88.4838, Test Loss: 0.9564, Test Accuracy: 89.9023\n",
      "Epoch [937/1000], Train Loss: 0.9713, Train Accuracy: 88.4838, Test Loss: 0.9564, Test Accuracy: 89.9023\n",
      "Epoch [938/1000], Train Loss: 0.9713, Train Accuracy: 88.4838, Test Loss: 0.9564, Test Accuracy: 89.9023\n",
      "Epoch [939/1000], Train Loss: 0.9713, Train Accuracy: 88.4838, Test Loss: 0.9564, Test Accuracy: 89.9023\n",
      "Epoch [940/1000], Train Loss: 0.9712, Train Accuracy: 88.4838, Test Loss: 0.9564, Test Accuracy: 89.9023\n",
      "Epoch [941/1000], Train Loss: 0.9712, Train Accuracy: 88.4838, Test Loss: 0.9563, Test Accuracy: 89.9023\n",
      "Epoch [942/1000], Train Loss: 0.9712, Train Accuracy: 88.4838, Test Loss: 0.9563, Test Accuracy: 89.9023\n",
      "Epoch [943/1000], Train Loss: 0.9712, Train Accuracy: 88.4838, Test Loss: 0.9563, Test Accuracy: 89.9023\n",
      "Epoch [944/1000], Train Loss: 0.9711, Train Accuracy: 88.4838, Test Loss: 0.9563, Test Accuracy: 89.9023\n",
      "Epoch [945/1000], Train Loss: 0.9711, Train Accuracy: 88.4838, Test Loss: 0.9563, Test Accuracy: 89.9023\n",
      "Epoch [946/1000], Train Loss: 0.9711, Train Accuracy: 88.4838, Test Loss: 0.9563, Test Accuracy: 89.9023\n",
      "Epoch [947/1000], Train Loss: 0.9711, Train Accuracy: 88.4838, Test Loss: 0.9562, Test Accuracy: 89.9023\n",
      "Epoch [948/1000], Train Loss: 0.9711, Train Accuracy: 88.4838, Test Loss: 0.9562, Test Accuracy: 89.9023\n",
      "Epoch [949/1000], Train Loss: 0.9710, Train Accuracy: 88.4838, Test Loss: 0.9562, Test Accuracy: 89.9023\n",
      "Epoch [950/1000], Train Loss: 0.9710, Train Accuracy: 88.4838, Test Loss: 0.9562, Test Accuracy: 89.9023\n",
      "Epoch [951/1000], Train Loss: 0.9710, Train Accuracy: 88.4838, Test Loss: 0.9562, Test Accuracy: 89.9023\n",
      "Epoch [952/1000], Train Loss: 0.9710, Train Accuracy: 88.4838, Test Loss: 0.9561, Test Accuracy: 89.9023\n",
      "Epoch [953/1000], Train Loss: 0.9710, Train Accuracy: 88.4838, Test Loss: 0.9561, Test Accuracy: 89.9023\n",
      "Epoch [954/1000], Train Loss: 0.9709, Train Accuracy: 88.4838, Test Loss: 0.9561, Test Accuracy: 89.9023\n",
      "Epoch [955/1000], Train Loss: 0.9709, Train Accuracy: 88.4838, Test Loss: 0.9561, Test Accuracy: 89.9023\n",
      "Epoch [956/1000], Train Loss: 0.9709, Train Accuracy: 88.4838, Test Loss: 0.9561, Test Accuracy: 89.9023\n",
      "Epoch [957/1000], Train Loss: 0.9709, Train Accuracy: 88.4838, Test Loss: 0.9561, Test Accuracy: 89.9023\n",
      "Epoch [958/1000], Train Loss: 0.9709, Train Accuracy: 88.4838, Test Loss: 0.9560, Test Accuracy: 89.9023\n",
      "Epoch [959/1000], Train Loss: 0.9708, Train Accuracy: 88.4838, Test Loss: 0.9560, Test Accuracy: 89.9023\n",
      "Epoch [960/1000], Train Loss: 0.9708, Train Accuracy: 88.4838, Test Loss: 0.9560, Test Accuracy: 89.9023\n",
      "Epoch [961/1000], Train Loss: 0.9708, Train Accuracy: 88.4838, Test Loss: 0.9560, Test Accuracy: 89.9023\n",
      "Epoch [962/1000], Train Loss: 0.9708, Train Accuracy: 88.4838, Test Loss: 0.9560, Test Accuracy: 89.9023\n",
      "Epoch [963/1000], Train Loss: 0.9708, Train Accuracy: 88.4838, Test Loss: 0.9560, Test Accuracy: 89.9023\n",
      "Epoch [964/1000], Train Loss: 0.9707, Train Accuracy: 88.4838, Test Loss: 0.9560, Test Accuracy: 89.9023\n",
      "Epoch [965/1000], Train Loss: 0.9707, Train Accuracy: 88.4838, Test Loss: 0.9559, Test Accuracy: 89.9023\n",
      "Epoch [966/1000], Train Loss: 0.9707, Train Accuracy: 88.4838, Test Loss: 0.9559, Test Accuracy: 89.9023\n",
      "Epoch [967/1000], Train Loss: 0.9707, Train Accuracy: 88.4838, Test Loss: 0.9559, Test Accuracy: 89.9023\n",
      "Epoch [968/1000], Train Loss: 0.9707, Train Accuracy: 88.4838, Test Loss: 0.9559, Test Accuracy: 89.9023\n",
      "Epoch [969/1000], Train Loss: 0.9706, Train Accuracy: 88.4838, Test Loss: 0.9559, Test Accuracy: 89.9023\n",
      "Epoch [970/1000], Train Loss: 0.9706, Train Accuracy: 88.4838, Test Loss: 0.9559, Test Accuracy: 89.9023\n",
      "Epoch [971/1000], Train Loss: 0.9706, Train Accuracy: 88.4838, Test Loss: 0.9558, Test Accuracy: 89.9023\n",
      "Epoch [972/1000], Train Loss: 0.9706, Train Accuracy: 88.4838, Test Loss: 0.9558, Test Accuracy: 89.9023\n",
      "Epoch [973/1000], Train Loss: 0.9706, Train Accuracy: 88.4838, Test Loss: 0.9558, Test Accuracy: 89.9023\n",
      "Epoch [974/1000], Train Loss: 0.9705, Train Accuracy: 88.4838, Test Loss: 0.9558, Test Accuracy: 89.9023\n",
      "Epoch [975/1000], Train Loss: 0.9705, Train Accuracy: 88.4838, Test Loss: 0.9558, Test Accuracy: 89.9023\n",
      "Epoch [976/1000], Train Loss: 0.9705, Train Accuracy: 88.4838, Test Loss: 0.9558, Test Accuracy: 89.9023\n",
      "Epoch [977/1000], Train Loss: 0.9705, Train Accuracy: 88.4838, Test Loss: 0.9557, Test Accuracy: 89.9023\n",
      "Epoch [978/1000], Train Loss: 0.9705, Train Accuracy: 88.4838, Test Loss: 0.9557, Test Accuracy: 89.9023\n",
      "Epoch [979/1000], Train Loss: 0.9704, Train Accuracy: 88.4838, Test Loss: 0.9557, Test Accuracy: 89.9023\n",
      "Epoch [980/1000], Train Loss: 0.9704, Train Accuracy: 88.4838, Test Loss: 0.9557, Test Accuracy: 89.9023\n",
      "Epoch [981/1000], Train Loss: 0.9704, Train Accuracy: 88.4838, Test Loss: 0.9557, Test Accuracy: 89.9023\n",
      "Epoch [982/1000], Train Loss: 0.9704, Train Accuracy: 88.4838, Test Loss: 0.9557, Test Accuracy: 89.9023\n",
      "Epoch [983/1000], Train Loss: 0.9704, Train Accuracy: 88.4838, Test Loss: 0.9557, Test Accuracy: 89.9023\n",
      "Epoch [984/1000], Train Loss: 0.9703, Train Accuracy: 88.4838, Test Loss: 0.9556, Test Accuracy: 89.9023\n",
      "Epoch [985/1000], Train Loss: 0.9703, Train Accuracy: 88.4838, Test Loss: 0.9556, Test Accuracy: 89.9023\n",
      "Epoch [986/1000], Train Loss: 0.9703, Train Accuracy: 88.4838, Test Loss: 0.9556, Test Accuracy: 89.9023\n",
      "Epoch [987/1000], Train Loss: 0.9703, Train Accuracy: 88.4838, Test Loss: 0.9556, Test Accuracy: 89.9023\n",
      "Epoch [988/1000], Train Loss: 0.9703, Train Accuracy: 88.4838, Test Loss: 0.9556, Test Accuracy: 89.9023\n",
      "Epoch [989/1000], Train Loss: 0.9702, Train Accuracy: 88.4838, Test Loss: 0.9556, Test Accuracy: 89.9023\n",
      "Epoch [990/1000], Train Loss: 0.9702, Train Accuracy: 88.4838, Test Loss: 0.9555, Test Accuracy: 89.9023\n",
      "Epoch [991/1000], Train Loss: 0.9702, Train Accuracy: 88.4838, Test Loss: 0.9555, Test Accuracy: 89.9023\n",
      "Epoch [992/1000], Train Loss: 0.9702, Train Accuracy: 88.4838, Test Loss: 0.9555, Test Accuracy: 89.9023\n",
      "Epoch [993/1000], Train Loss: 0.9702, Train Accuracy: 88.4838, Test Loss: 0.9555, Test Accuracy: 89.9023\n",
      "Epoch [994/1000], Train Loss: 0.9701, Train Accuracy: 88.4838, Test Loss: 0.9555, Test Accuracy: 89.9023\n",
      "Epoch [995/1000], Train Loss: 0.9701, Train Accuracy: 88.4838, Test Loss: 0.9555, Test Accuracy: 89.9023\n",
      "Epoch [996/1000], Train Loss: 0.9701, Train Accuracy: 88.4838, Test Loss: 0.9555, Test Accuracy: 89.9023\n",
      "Epoch [997/1000], Train Loss: 0.9701, Train Accuracy: 88.4838, Test Loss: 0.9554, Test Accuracy: 89.9023\n",
      "Epoch [998/1000], Train Loss: 0.9701, Train Accuracy: 88.4838, Test Loss: 0.9554, Test Accuracy: 89.9023\n",
      "Epoch [999/1000], Train Loss: 0.9701, Train Accuracy: 88.4838, Test Loss: 0.9554, Test Accuracy: 89.9023\n",
      "Epoch [1000/1000], Train Loss: 0.9700, Train Accuracy: 88.4838, Test Loss: 0.9554, Test Accuracy: 89.9023\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    train_loss = criterion(outputs, y_train)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_accuracy = calculate_accuracy(outputs, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "        test_accuracy = calculate_accuracy(test_outputs, y_test)\n",
    "\n",
    "    # Store losses and accuracies\n",
    "        # Store losses and accuracies\n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Train Accuracy: {train_accuracy.item():.4f}, Test Loss: {test_loss.item():.4f}, Test Accuracy: {test_accuracy.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      1.00      0.95       276\n",
      "         1.0       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.90       307\n",
      "   macro avg       0.45      0.50      0.47       307\n",
      "weighted avg       0.81      0.90      0.85       307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/usama/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/usama/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/usama/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# printing classification report\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    test_predictions = test_outputs.round().numpy()\n",
    "    print(f'Classification Report:\\n{classification_report(y_test.numpy(), test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model is predicting the majority class only.\n",
    "#### Our Data is imbalance \n",
    "#### Adding more data will lead to better learning and accuracies\n",
    "#### Techniques like Stratified k-fold cross-validation and Weighted Random sampling can be used to cater class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usama",
   "language": "python",
   "name": "usama"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
